{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD SUMMARY (before removing Unnamed: 0 column):\n",
    "\n",
    "- Average accuracy of **XGBClassifier** is = 87.42%\n",
    "- Average accuracy of **LGBMClassifier** is = 86.45%\n",
    "- Average accuracy of **Random Forest Classifier** is = 84.9%\n",
    "- Average accuracy of **Extra Trees Classifier** is =  84.25%\n",
    "- Average accuracy of **Logistic Regression** is = 79.96%\n",
    "\n",
    "#### AVERAGE ACCURACY OF OUR ENSEMBLE MODEL = 84.6%\n",
    "\n",
    "\n",
    "### NEW SUMMARY:\n",
    "\n",
    "- Average accuracy of **XGBClassifier** is = 73.75%\n",
    "- Average accuracy of **LGBMClassifier** is = 75%\n",
    "- Average accuracy of **Random Forest Classifier** is = 75.44%\n",
    "- Average accuracy of **Extra Trees Classifier** is =  74.52%\n",
    "- Average accuracy of **Logistic Regression** is = 75.62%\n",
    "\n",
    "#### AVERAGE ACCURACY OF OUR ENSEMBLE MODEL = 74.86%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from numpy import mean\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoder = OneHotEncoder()\n",
    "label_encoder = LabelEncoder()\n",
    "ordinal = OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## EXTRA TREES CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X, y)\n",
    "# #X_heroku = ordinal.fit_transform(pd.DataFrame([[1,1,1,1,1,1]])) #[5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5]\n",
    "# model.predict_proba([[5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for PQ_G1 is 91.18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('PQ_G1.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2','PAG_NAME_3','PAG_NAME_4'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=5, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "print('Accuracy for PQ_G1 is',round((100*mean(n_scores)),2))\n",
    "accs.append(100*mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ETC_PQ_G1.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  34   35   45   66   86  141  150  156  165  175  185  195  199  208\n",
      "  221  226  228  233  243  277  292  309  380  440  483  492  505  543\n",
      "  593  621  631  646  650  651  661  673  703  709  725  736  738  755\n",
      "  782  790  807  810  871  881  895  922  948  950  957  968  982  996\n",
      " 1010 1029 1048 1050 1087 1105 1145 1159 1177 1179 1185 1206 1210 1224\n",
      " 1239 1245 1252 1292 1325 1331 1333 1349 1353 1355 1374 1391 1397 1414\n",
      " 1442 1450 1460 1467 1469 1533 1547 1587 1594 1621 1631 1642 1664 1666\n",
      " 1680 1699 1717 1723 1732 1739 1761 1772 1785 1795 1797 1799 1819 1846\n",
      " 1855 1858 1868 1869 1887 1903 1905 1920 1926 1935 1949 1957 1975 1981\n",
      " 1989 2011 2014 2040 2073 2076 2093 2117 2131 2135 2140 2183 2205 2208\n",
      " 2223 2243 2271 2289 2299 2303 2304 2324 2333 2335 2375 2385 2411 2414\n",
      " 2428 2457 2465 2492 2494 2510 2512 2574 2587 2589 2591 2592 2594 2620\n",
      " 2643 2650 2658 2662 2672 2675 2678 2694 2700 2703 2717 2726 2728 2735\n",
      " 2737 2771 2806 2833 2834 2845 2862 2875 2880 2938 2956 2984 2986 3013\n",
      " 3029 3032 3044 3052 3084 3096 3120 3122 3129 3147 3148 3161 3170 3185\n",
      " 3188 3195 3202 3232 3246 3251 3264 3272 3287 3299 3328 3352 3356 3401\n",
      " 3414 3416 3424 3430 3438 3462 3472 3473 3482 3484 3502 3503 3506 3547\n",
      " 3565 3568 3607 3630 3641 3665 3667 3689 3710 3714 3721 3739]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  12   13   26   27   68   88   93  110  125  127  204  213  229  239\n",
      "  257  285  301  308  315  332  336  339  351  364  373  375  394  427\n",
      "  437  453  486  507  526  530  534  537  580  600  603  608  630  636\n",
      "  640  685  687  690  691  702  706  732  747  753  757  770  783  785\n",
      "  786  820  854  857  869  875  879  883  890  893  914  926  976 1055\n",
      " 1060 1067 1071 1072 1084 1088 1091 1125 1138 1191 1216 1231 1243 1270\n",
      " 1298 1300 1301 1319 1338 1343 1344 1350 1351 1364 1400 1404 1452 1468\n",
      " 1492 1494 1524 1531 1536 1546 1548 1556 1562 1566 1596 1611 1649 1705\n",
      " 1733 1757 1758 1788 1803 1823 1825 1842 1864 1866 1870 1899 1912 1922\n",
      " 1924 1929 1950 1984 1986 1993 1995 2010 2051 2072 2074 2083 2121 2126\n",
      " 2142 2162 2170 2175 2188 2201 2249 2259 2261 2262 2292 2296 2311 2329\n",
      " 2337 2344 2349 2350 2362 2410 2412 2435 2445 2471 2493 2496 2516 2529\n",
      " 2530 2595 2611 2627 2634 2655 2677 2680 2683 2701 2704 2713 2714 2720\n",
      " 2730 2739 2742 2760 2776 2784 2786 2796 2801 2814 2825 2836 2847 2851\n",
      " 2857 2884 2914 2920 2940 2941 2950 2981 3022 3036 3041 3049 3070 3081\n",
      " 3104 3124 3133 3141 3155 3157 3247 3274 3279 3322 3336 3355 3360 3363\n",
      " 3396 3400 3427 3440 3449 3452 3467 3477 3480 3481 3488 3491 3499 3521\n",
      " 3540 3558 3561 3570 3584 3602 3619 3640 3656 3690 3702 3743]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  10   19   25   31   33   49   61   65  103  118  130  132  142  153\n",
      "  158  211  220  222  230  248  270  282  284  300  353  354  355  357\n",
      "  360  374  376  399  411  426  460  479  485  488  491  500  510  518\n",
      "  569  574  575  595  620  667  669  693  696  710  730  734  791  796\n",
      "  816  825  841  853  863  870  876  880  894  902  917  927  958  972\n",
      "  979 1018 1019 1080 1102 1113 1123 1178 1244 1256 1272 1289 1306 1309\n",
      " 1311 1320 1330 1373 1385 1412 1416 1434 1437 1446 1457 1461 1473 1480\n",
      " 1514 1523 1543 1573 1574 1579 1584 1604 1627 1639 1640 1645 1669 1676\n",
      " 1677 1691 1706 1711 1715 1729 1749 1769 1778 1787 1834 1851 1867 1880\n",
      " 1884 1897 1909 1946 1951 1960 1973 1983 1988 2009 2039 2042 2055 2084\n",
      " 2088 2119 2122 2144 2154 2161 2197 2215 2225 2238 2252 2265 2283 2295\n",
      " 2323 2334 2343 2353 2360 2364 2370 2374 2377 2388 2396 2409 2416 2424\n",
      " 2434 2438 2468 2482 2495 2505 2525 2531 2551 2570 2609 2629 2659 2665\n",
      " 2685 2687 2711 2731 2792 2803 2818 2822 2827 2858 2865 2872 2895 2898\n",
      " 2901 2913 2927 2930 2958 2972 3010 3014 3021 3028 3040 3048 3080 3089\n",
      " 3090 3095 3143 3149 3166 3190 3193 3229 3233 3250 3254 3288 3290 3309\n",
      " 3321 3333 3381 3384 3425 3445 3463 3464 3468 3498 3518 3532 3541 3543\n",
      " 3582 3589 3597 3598 3668 3671 3674 3694 3701 3711 3713 3741]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [   4    5   16   41   51   55   84  147  152  180  219  232  241  245\n",
      "  259  268  275  276  280  299  320  330  335  341  363  418  421  428\n",
      "  433  434  466  467  473  531  540  547  551  559  560  570  579  588\n",
      "  589  606  626  648  655  656  658  684  692  711  720  724  729  749\n",
      "  754  769  778  781  794  802  805  813  843  868  882  888  916  921\n",
      "  933  944  955  962  971  987 1017 1034 1035 1073 1107 1111 1132 1141\n",
      " 1170 1175 1223 1225 1233 1271 1277 1312 1336 1339 1366 1371 1387 1393\n",
      " 1394 1413 1421 1430 1481 1484 1485 1486 1488 1491 1496 1516 1530 1578\n",
      " 1582 1598 1613 1622 1623 1624 1626 1651 1652 1662 1667 1742 1746 1750\n",
      " 1783 1784 1800 1810 1814 1817 1838 1845 1859 1872 1878 1881 1948 1952\n",
      " 1953 1965 1990 1996 2003 2017 2020 2025 2057 2069 2071 2078 2080 2097\n",
      " 2113 2159 2165 2174 2204 2220 2230 2246 2251 2255 2287 2313 2389 2394\n",
      " 2405 2449 2450 2466 2473 2504 2526 2537 2547 2554 2583 2616 2626 2732\n",
      " 2745 2749 2761 2772 2778 2797 2912 2929 2939 2944 2949 2975 2985 2993\n",
      " 3002 3005 3047 3072 3094 3107 3111 3116 3140 3154 3173 3174 3189 3194\n",
      " 3199 3204 3213 3217 3262 3266 3283 3303 3306 3307 3320 3325 3349 3350\n",
      " 3364 3371 3382 3433 3434 3443 3444 3465 3497 3507 3511 3525 3530 3534\n",
      " 3552 3556 3620 3635 3700 3703 3705 3715 3717 3722 3727 3729]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  18   28   29   32   46   57   71   79   90  113  159  172  178  201\n",
      "  217  225  252  302  318  358  400  409  414  422  429  445  450  457\n",
      "  463  468  470  474  494  521  528  532  533  554  573  576  643  672\n",
      "  675  707  708  741  760  761  784  812  814  824  830  831  861  934\n",
      "  935  949  960  961  991 1026 1036 1049 1068 1070 1079 1085 1089 1096\n",
      " 1116 1118 1119 1133 1137 1149 1161 1171 1173 1184 1193 1197 1214 1226\n",
      " 1230 1236 1250 1274 1276 1284 1286 1313 1322 1329 1345 1365 1372 1377\n",
      " 1379 1399 1410 1462 1470 1489 1504 1510 1528 1544 1549 1567 1586 1603\n",
      " 1633 1656 1663 1671 1678 1696 1704 1712 1714 1734 1752 1793 1849 1883\n",
      " 1927 1976 1987 2001 2005 2026 2045 2077 2095 2101 2125 2127 2155 2169\n",
      " 2172 2176 2180 2187 2195 2199 2202 2213 2217 2229 2242 2258 2272 2340\n",
      " 2387 2443 2463 2470 2475 2477 2479 2484 2491 2523 2540 2543 2553 2558\n",
      " 2566 2568 2573 2576 2593 2622 2635 2638 2645 2656 2660 2692 2722 2723\n",
      " 2744 2753 2770 2785 2813 2816 2821 2824 2829 2854 2877 2889 2905 2924\n",
      " 2945 2961 3012 3020 3034 3035 3039 3058 3059 3060 3061 3075 3093 3113\n",
      " 3125 3142 3144 3160 3191 3203 3220 3235 3236 3241 3269 3292 3313 3358\n",
      " 3376 3390 3399 3428 3429 3446 3459 3466 3487 3526 3528 3567 3571 3574\n",
      " 3621 3633 3644 3651 3655 3659 3661 3688 3695 3735 3740 3744]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  78   94  108  111  119  128  136  144  162  163  179  212  242  249\n",
      "  264  265  283  296  304  322  334  337  356  359  361  379  402  405\n",
      "  415  442  443  476  484  495  562  572  584  590  604  665  670  674\n",
      "  678  681  695  699  719  726  737  743  746  748  750  772  773  832\n",
      "  836  838  840  847  848  889  897  974 1001 1007 1013 1021 1023 1081\n",
      " 1095 1100 1106 1109 1117 1142 1156 1176 1196 1203 1212 1215 1254 1264\n",
      " 1288 1296 1303 1324 1359 1427 1431 1453 1456 1463 1487 1501 1563 1575\n",
      " 1599 1638 1647 1654 1675 1682 1687 1694 1698 1716 1724 1759 1792 1796\n",
      " 1798 1808 1811 1824 1837 1844 1852 1876 1885 1892 1898 1906 1945 1955\n",
      " 1956 1971 2008 2024 2028 2031 2046 2064 2081 2102 2104 2106 2115 2120\n",
      " 2150 2164 2182 2203 2207 2210 2221 2226 2269 2277 2307 2354 2365 2376\n",
      " 2383 2395 2407 2421 2427 2447 2485 2517 2555 2559 2571 2577 2581 2585\n",
      " 2628 2632 2636 2663 2679 2684 2690 2697 2715 2750 2762 2767 2779 2804\n",
      " 2823 2860 2881 2882 2886 2911 2916 2918 2921 2928 2936 2942 2989 2995\n",
      " 3001 3024 3026 3063 3068 3073 3074 3139 3150 3152 3179 3182 3183 3211\n",
      " 3240 3248 3261 3284 3286 3289 3301 3315 3317 3340 3344 3345 3346 3353\n",
      " 3361 3367 3369 3393 3418 3439 3474 3475 3495 3500 3535 3542 3550 3554\n",
      " 3555 3564 3566 3575 3577 3583 3594 3604 3624 3657 3718 3736]\n",
      "TRAIN: [   0    1    2 ... 3744 3745 3747] TEST: [   6   38   72  109  148  151  174  210  215  216  224  235  260  289\n",
      "  303  349  372  389  395  404  406  431  432  458  459  503  513  515\n",
      "  522  539  556  558  568  582  599  610  611  616  624  628  629  632\n",
      "  683  808  819  826  827  852  856  866  867  874  885  911  929  930\n",
      "  951  954  967  970  978  985  999 1002 1005 1016 1025 1075 1104 1136\n",
      " 1181 1186 1188 1237 1240 1241 1251 1262 1267 1302 1305 1310 1342 1361\n",
      " 1369 1390 1401 1420 1435 1436 1459 1495 1532 1537 1558 1581 1593 1605\n",
      " 1606 1607 1629 1636 1650 1653 1684 1700 1727 1731 1740 1753 1780 1781\n",
      " 1794 1804 1807 1833 1871 1879 1917 1958 1992 2004 2006 2007 2027 2044\n",
      " 2049 2060 2062 2065 2068 2075 2105 2112 2114 2139 2146 2151 2173 2193\n",
      " 2206 2216 2222 2241 2245 2270 2282 2286 2293 2341 2351 2373 2384 2399\n",
      " 2432 2440 2448 2454 2455 2488 2506 2513 2514 2556 2562 2580 2597 2605\n",
      " 2614 2625 2639 2641 2666 2670 2671 2691 2696 2706 2709 2746 2769 2773\n",
      " 2774 2783 2808 2812 2838 2852 2878 2887 2896 2897 2902 2919 2922 2923\n",
      " 2943 2946 2959 2978 2980 2983 3045 3066 3086 3105 3108 3130 3134 3159\n",
      " 3164 3168 3212 3224 3238 3267 3275 3280 3302 3310 3319 3343 3359 3372\n",
      " 3373 3409 3442 3453 3456 3476 3478 3479 3483 3489 3493 3514 3517 3520\n",
      " 3531 3572 3585 3586 3637 3675 3692 3708 3723 3724 3733 3746]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  36   40   59   62   92  107  138  139  143  155  160  166  186  187\n",
      "  191  203  207  209  227  246  262  266  310  313  331  342  343  346\n",
      "  347  348  378  383  417  454  462  477  478  480  489  525  529  550\n",
      "  561  566  571  578  591  597  605  612  619  625  627  635  642  671\n",
      "  677  680  689  694  716  731  733  740  742  744  764  777  803  811\n",
      "  817  822  828  860  884  903  906  915  928  931  942  965  977 1063\n",
      " 1069 1078 1098 1115 1121 1134 1144 1150 1152 1153 1154 1160 1168 1172\n",
      " 1198 1227 1247 1249 1255 1261 1278 1287 1294 1304 1317 1326 1363 1418\n",
      " 1445 1478 1482 1493 1517 1521 1538 1545 1553 1592 1600 1610 1661 1730\n",
      " 1766 1774 1790 1805 1841 1850 1863 1925 1934 2015 2016 2019 2036 2047\n",
      " 2059 2066 2103 2133 2149 2152 2166 2191 2192 2194 2214 2237 2240 2266\n",
      " 2273 2301 2308 2318 2326 2327 2332 2361 2368 2386 2393 2406 2433 2441\n",
      " 2456 2462 2469 2487 2489 2519 2572 2599 2601 2603 2604 2608 2610 2623\n",
      " 2649 2695 2708 2747 2756 2757 2782 2789 2799 2810 2815 2820 2841 2867\n",
      " 2874 2888 2908 2935 2951 2977 2982 3003 3008 3017 3025 3027 3053 3054\n",
      " 3083 3100 3135 3145 3146 3158 3163 3206 3230 3231 3252 3258 3260 3273\n",
      " 3291 3385 3391 3394 3403 3408 3469 3471 3494 3510 3519 3536 3538 3539\n",
      " 3560 3614 3615 3627 3634 3660 3679 3682 3687 3693 3716 3732]\n",
      "TRAIN: [   0    1    4 ... 3745 3746 3747] TEST: [   2    3    8   17   20   83   91   98   99  115  123  154  157  164\n",
      "  173  192  218  231  237  238  286  287  293  298  306  319  323  327\n",
      "  345  397  413  430  441  448  451  482  501  506  538  548  552  553\n",
      "  567  592  594  602  615  639  649  652  653  676  700  704  715  721\n",
      "  768  799  806  815  818  834  837  845  849  859  864  872  887  898\n",
      "  924  981  994  997 1004 1033 1040 1044 1046 1047 1053 1056 1059 1127\n",
      " 1140 1182 1187 1192 1211 1221 1334 1340 1348 1388 1396 1398 1408 1439\n",
      " 1449 1455 1476 1477 1490 1507 1551 1555 1560 1576 1591 1597 1628 1655\n",
      " 1668 1688 1702 1721 1722 1751 1755 1771 1776 1782 1789 1809 1822 1839\n",
      " 1843 1861 1896 1900 1902 1907 1913 1916 1933 1940 1980 2000 2063 2085\n",
      " 2089 2096 2098 2116 2129 2163 2186 2198 2227 2247 2263 2267 2274 2278\n",
      " 2298 2300 2314 2322 2330 2338 2346 2358 2415 2419 2442 2478 2501 2520\n",
      " 2521 2533 2534 2542 2544 2584 2588 2654 2676 2689 2705 2725 2727 2729\n",
      " 2736 2741 2766 2780 2781 2830 2835 2840 2843 2844 2859 2864 2899 2903\n",
      " 2906 2910 2948 2997 3015 3030 3037 3050 3091 3092 3102 3110 3175 3177\n",
      " 3207 3216 3218 3223 3256 3276 3297 3330 3388 3389 3395 3398 3402 3407\n",
      " 3411 3417 3419 3437 3441 3447 3451 3470 3522 3569 3590 3591 3595 3596\n",
      " 3618 3625 3631 3638 3648 3649 3663 3680 3706 3726 3728 3730]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  22   23   63   73   97  101  126  133  145  176  181  206  261  273\n",
      "  274  295  305  307  312  321  329  338  340  344  366  367  369  388\n",
      "  398  407  412  416  425  444  455  464  469  472  517  545  549  587\n",
      "  598  607  609  634  641  644  705  713  717  745  758  766  787  801\n",
      "  823  833  901  908  918  919  925  938  943  947  956  959  966  980\n",
      " 1000 1015 1027 1039 1043 1051 1062 1064 1076 1077 1090 1101 1103 1114\n",
      " 1169 1201 1204 1246 1248 1258 1266 1283 1291 1295 1307 1332 1367 1370\n",
      " 1382 1389 1415 1424 1432 1438 1451 1499 1508 1542 1561 1569 1614 1635\n",
      " 1665 1681 1710 1728 1737 1747 1754 1763 1764 1816 1827 1828 1848 1856\n",
      " 1882 1964 1972 1977 1994 1998 2023 2041 2111 2123 2130 2145 2177 2211\n",
      " 2239 2250 2279 2280 2305 2312 2315 2331 2367 2372 2378 2382 2418 2444\n",
      " 2476 2497 2503 2515 2532 2535 2538 2541 2546 2565 2575 2600 2606 2615\n",
      " 2631 2644 2657 2668 2674 2688 2721 2751 2787 2791 2798 2800 2828 2831\n",
      " 2846 2853 2856 2863 2879 2904 2915 2934 2952 2954 2967 2969 2970 2974\n",
      " 2987 2990 3009 3067 3071 3076 3103 3112 3115 3131 3136 3153 3156 3184\n",
      " 3186 3196 3249 3253 3282 3285 3294 3295 3296 3311 3318 3331 3362 3365\n",
      " 3370 3379 3405 3413 3423 3436 3458 3460 3523 3527 3557 3587 3592 3600\n",
      " 3605 3612 3629 3636 3647 3650 3653 3658 3662 3699 3731 3734]\n",
      "TRAIN: [   1    2    3 ... 3744 3745 3746] TEST: [   0   15   42   48   58   70  105  114  120  131  140  146  168  188\n",
      "  200  223  244  247  254  255  294  365  403  420  435  446  449  456\n",
      "  509  519  536  557  581  583  585  586  596  613  614  647  663  698\n",
      "  712  751  771  774  775  795  800  809  846  913  920  923  946  964\n",
      "  975  986  993 1003 1006 1012 1020 1022 1037 1042 1045 1052 1058 1066\n",
      " 1092 1130 1163 1167 1174 1180 1208 1209 1273 1279 1280 1293 1327 1337\n",
      " 1346 1347 1356 1360 1368 1395 1402 1403 1411 1428 1429 1447 1448 1464\n",
      " 1479 1483 1500 1506 1535 1559 1565 1585 1589 1619 1634 1637 1644 1670\n",
      " 1673 1695 1707 1719 1725 1744 1768 1770 1773 1779 1821 1831 1873 1875\n",
      " 1877 1886 1890 1895 1914 1928 1931 1962 2013 2033 2034 2038 2070 2086\n",
      " 2090 2108 2124 2137 2157 2167 2184 2189 2209 2233 2253 2276 2290 2291\n",
      " 2294 2302 2310 2357 2363 2369 2371 2379 2380 2397 2404 2430 2452 2461\n",
      " 2467 2472 2474 2483 2498 2500 2528 2539 2552 2560 2617 2618 2699 2702\n",
      " 2710 2754 2788 2795 2805 2809 2817 2890 2891 2907 2955 2957 2996 3004\n",
      " 3006 3018 3038 3043 3051 3069 3077 3079 3126 3137 3138 3151 3162 3169\n",
      " 3181 3222 3227 3228 3234 3243 3255 3265 3268 3270 3271 3281 3314 3324\n",
      " 3338 3351 3392 3397 3410 3421 3422 3432 3490 3504 3512 3516 3544 3559\n",
      " 3576 3601 3611 3613 3639 3666 3669 3673 3676 3684 3686 3747]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  44   56   64   75  106  117  121  134  183  184  189  194  197  202\n",
      "  234  250  251  267  269  272  316  324  325  368  371  384  385  410\n",
      "  423  438  447  461  471  487  490  496  498  499  514  516  541  542\n",
      "  546  637  654  662  664  688  697  701  722  767  788  792  797  851\n",
      "  855  858  873  900  909  912  932  936  941  969 1009 1011 1028 1032\n",
      " 1074 1086 1143 1157 1158 1162 1164 1183 1190 1195 1200 1207 1218 1234\n",
      " 1257 1269 1290 1314 1323 1335 1362 1375 1383 1384 1426 1474 1513 1520\n",
      " 1525 1526 1539 1552 1572 1580 1588 1608 1689 1692 1693 1697 1735 1736\n",
      " 1748 1760 1767 1791 1801 1806 1813 1832 1836 1862 1888 1891 1901 1910\n",
      " 1939 1943 1954 1970 1978 1979 2043 2053 2056 2067 2082 2087 2092 2094\n",
      " 2099 2100 2107 2109 2128 2138 2153 2156 2171 2244 2257 2281 2297 2309\n",
      " 2347 2390 2391 2392 2413 2437 2451 2458 2499 2502 2509 2511 2524 2545\n",
      " 2563 2569 2612 2613 2619 2621 2640 2642 2664 2669 2673 2693 2698 2707\n",
      " 2743 2752 2758 2763 2794 2807 2811 2869 2893 2933 2953 2960 2962 2965\n",
      " 2966 2971 2979 2991 2994 2998 3007 3031 3042 3065 3087 3088 3098 3109\n",
      " 3128 3198 3201 3205 3226 3237 3245 3293 3323 3334 3337 3348 3368 3383\n",
      " 3386 3387 3404 3406 3412 3431 3450 3457 3492 3505 3515 3537 3573 3588\n",
      " 3606 3608 3617 3626 3664 3670 3672 3681 3685 3696 3709 3719]\n",
      "TRAIN: [   0    1    2 ... 3744 3746 3747] TEST: [   9   11   24   30   43   52   54   76   80   81   87   89  104  124\n",
      "  129  149  171  240  271  278  281  290  291  326  328  333  370  381\n",
      "  387  396  401  419  465  520  524  535  577  601  622  633  668  686\n",
      "  714  718  727  739  752  759  762  789  839  844  910  939  953  988\n",
      " 1030 1061 1065 1082 1093 1094 1128 1135 1139 1155 1194 1202 1213 1217\n",
      " 1238 1242 1259 1265 1299 1315 1352 1358 1378 1380 1386 1433 1440 1443\n",
      " 1454 1465 1466 1472 1497 1505 1509 1511 1522 1527 1564 1568 1570 1571\n",
      " 1601 1609 1617 1620 1625 1630 1660 1679 1683 1686 1703 1708 1709 1720\n",
      " 1726 1741 1743 1762 1812 1815 1826 1829 1830 1847 1860 1865 1894 1908\n",
      " 1911 1915 1936 1941 1944 1947 1959 1967 1991 2002 2021 2030 2032 2050\n",
      " 2079 2136 2141 2143 2147 2196 2200 2219 2224 2228 2234 2235 2236 2248\n",
      " 2275 2284 2288 2320 2325 2336 2345 2348 2356 2359 2366 2400 2408 2423\n",
      " 2426 2429 2439 2464 2480 2481 2490 2507 2508 2536 2548 2561 2564 2586\n",
      " 2590 2602 2624 2637 2653 2686 2724 2738 2755 2759 2764 2765 2768 2775\n",
      " 2777 2802 2819 2826 2832 2837 2850 2868 2870 2871 2873 2894 2900 2909\n",
      " 2926 2947 2963 2973 3055 3085 3097 3101 3171 3172 3192 3197 3219 3244\n",
      " 3259 3300 3312 3326 3327 3347 3454 3461 3508 3513 3546 3549 3551 3553\n",
      " 3563 3580 3616 3623 3654 3683 3691 3698 3725 3737 3738 3745]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  21   37   50   53   60   67   69   77   85   95   96  100  102  135\n",
      "  161  196  198  214  236  256  258  263  279  297  362  377  382  391\n",
      "  393  408  424  436  502  504  512  523  527  555  564  565  617  623\n",
      "  638  645  657  659  660  679  682  728  756  776  779  780  804  850\n",
      "  862  865  886  891  896  904  937  940  963  984  989  990  995 1008\n",
      " 1038 1054 1083 1097 1112 1124 1126 1129 1131 1147 1151 1189 1199 1219\n",
      " 1220 1222 1228 1229 1232 1235 1260 1268 1285 1316 1321 1328 1354 1357\n",
      " 1392 1409 1419 1422 1425 1441 1444 1458 1503 1512 1518 1519 1529 1534\n",
      " 1540 1541 1550 1557 1577 1583 1590 1602 1612 1616 1618 1641 1648 1657\n",
      " 1659 1672 1685 1713 1738 1745 1802 1818 1835 1840 1853 1854 1874 1893\n",
      " 1919 1921 1923 1932 1942 1961 1966 1968 1969 1974 1985 1999 2012 2037\n",
      " 2048 2054 2132 2134 2158 2160 2168 2190 2231 2260 2264 2268 2306 2317\n",
      " 2321 2328 2339 2381 2401 2402 2422 2431 2446 2459 2460 2518 2527 2549\n",
      " 2596 2630 2651 2652 2661 2667 2712 2719 2740 2790 2842 2848 2849 2861\n",
      " 2885 2925 2932 2964 2976 2992 3000 3011 3019 3033 3056 3062 3064 3082\n",
      " 3106 3117 3119 3180 3187 3208 3209 3210 3239 3242 3257 3277 3305 3308\n",
      " 3329 3332 3339 3374 3377 3415 3420 3455 3485 3486 3533 3548 3578 3579\n",
      " 3581 3593 3603 3622 3628 3645 3646 3704 3707 3712 3720]\n",
      "TRAIN: [   0    2    3 ... 3745 3746 3747] TEST: [   1    7   14   39   47   74   82  112  116  122  137  167  169  170\n",
      "  177  182  190  193  205  253  288  311  314  317  350  352  386  390\n",
      "  392  439  452  475  481  493  497  508  511  544  563  618  666  723\n",
      "  735  763  765  793  798  821  829  835  842  877  878  892  899  905\n",
      "  907  945  952  973  983  992  998 1014 1024 1031 1041 1057 1099 1108\n",
      " 1110 1120 1122 1146 1148 1165 1166 1205 1253 1263 1275 1281 1282 1297\n",
      " 1308 1318 1341 1376 1381 1405 1406 1407 1417 1423 1471 1475 1498 1502\n",
      " 1515 1554 1595 1615 1632 1643 1646 1658 1674 1690 1701 1718 1756 1765\n",
      " 1775 1777 1786 1820 1857 1889 1904 1918 1930 1937 1938 1963 1982 1997\n",
      " 2018 2022 2029 2035 2052 2058 2061 2091 2110 2118 2148 2178 2179 2181\n",
      " 2185 2212 2218 2232 2254 2256 2285 2316 2319 2342 2352 2355 2398 2403\n",
      " 2417 2420 2425 2436 2453 2486 2522 2550 2557 2567 2578 2579 2582 2598\n",
      " 2607 2633 2646 2647 2648 2681 2682 2716 2718 2733 2734 2748 2793 2839\n",
      " 2855 2866 2876 2883 2892 2917 2931 2937 2968 2988 2999 3016 3023 3046\n",
      " 3057 3078 3099 3114 3118 3121 3123 3127 3132 3165 3167 3176 3178 3200\n",
      " 3214 3215 3221 3225 3263 3278 3298 3304 3316 3335 3341 3342 3354 3357\n",
      " 3366 3375 3378 3380 3426 3435 3448 3496 3501 3509 3524 3529 3545 3562\n",
      " 3599 3609 3610 3632 3642 3643 3652 3677 3678 3697 3742]\n",
      "TRAIN: [   0    1    3 ... 3745 3746 3747] TEST: [   2   12   21   25   60   90   91   95   96  105  109  113  118  138\n",
      "  170  212  227  231  237  255  257  263  266  291  311  312  314  329\n",
      "  346  389  410  437  442  453  454  456  464  468  483  530  538  545\n",
      "  550  587  591  616  618  626  645  671  675  681  687  721  726  737\n",
      "  766  768  772  786  799  800  805  811  827  836  837  847  891  893\n",
      "  894  897  898  900  902  933  947  949  950  990 1010 1050 1054 1056\n",
      " 1069 1084 1105 1120 1152 1160 1217 1244 1275 1317 1359 1362 1367 1383\n",
      " 1386 1391 1418 1430 1560 1591 1599 1624 1628 1631 1642 1650 1670 1690\n",
      " 1703 1707 1729 1812 1857 1862 1876 1881 1916 1918 1920 1923 1935 1936\n",
      " 1941 1955 1999 2017 2036 2037 2042 2054 2057 2071 2092 2108 2122 2125\n",
      " 2129 2169 2197 2212 2224 2233 2256 2266 2273 2281 2305 2322 2327 2359\n",
      " 2363 2366 2375 2377 2379 2390 2393 2399 2425 2427 2434 2436 2440 2443\n",
      " 2459 2475 2476 2479 2483 2494 2510 2525 2559 2566 2591 2600 2621 2644\n",
      " 2670 2698 2700 2706 2707 2711 2740 2803 2851 2887 2902 2905 2910 2912\n",
      " 2920 2939 2961 2996 3002 3008 3016 3018 3028 3068 3076 3106 3107 3111\n",
      " 3115 3118 3131 3141 3154 3163 3169 3202 3214 3232 3236 3244 3281 3284\n",
      " 3303 3314 3317 3328 3403 3409 3437 3443 3498 3552 3557 3567 3575 3590\n",
      " 3606 3608 3609 3611 3621 3630 3635 3657 3678 3697 3722 3730]\n",
      "TRAIN: [   0    1    2 ... 3744 3746 3747] TEST: [  13   26   35   38   86   88  106  117  132  145  159  176  179  180\n",
      "  184  195  199  200  215  221  233  247  294  326  330  339  340  347\n",
      "  350  360  368  378  381  406  438  450  461  472  522  526  533  551\n",
      "  553  559  593  606  674  705  719  722  767  776  779  791  795  816\n",
      "  819  825  829  832  835  843  854  866  879  896  908  916  924  973\n",
      "  978  979 1017 1027 1059 1075 1092 1108 1126 1131 1143 1161 1163 1206\n",
      " 1207 1226 1228 1239 1247 1250 1255 1257 1259 1278 1298 1301 1326 1327\n",
      " 1329 1337 1369 1373 1388 1427 1428 1447 1464 1466 1468 1474 1503 1508\n",
      " 1536 1544 1563 1566 1575 1590 1632 1645 1649 1674 1683 1687 1722 1732\n",
      " 1774 1795 1806 1808 1811 1845 1885 1891 1906 1952 1983 2001 2020 2055\n",
      " 2069 2096 2103 2123 2132 2133 2147 2153 2186 2193 2194 2203 2204 2219\n",
      " 2236 2264 2338 2362 2446 2454 2455 2460 2496 2503 2522 2533 2547 2551\n",
      " 2553 2579 2594 2617 2632 2642 2647 2649 2651 2654 2662 2690 2726 2744\n",
      " 2754 2764 2779 2789 2813 2824 2829 2863 2882 2888 2921 2928 2944 2965\n",
      " 2985 2986 2994 2997 3006 3030 3050 3052 3101 3108 3134 3135 3136 3146\n",
      " 3177 3182 3200 3201 3208 3223 3254 3279 3288 3313 3369 3377 3379 3388\n",
      " 3399 3401 3415 3440 3446 3448 3462 3470 3488 3500 3508 3510 3519 3522\n",
      " 3553 3587 3589 3601 3605 3620 3644 3654 3716 3720 3740 3745]\n",
      "TRAIN: [   1    2    3 ... 3745 3746 3747] TEST: [   0   24   37   45   64   67   69   72   75   82  108  147  192  198\n",
      "  207  220  239  241  259  273  315  318  349  353  355  393  402  412\n",
      "  414  415  427  433  446  458  460  465  469  514  520  542  581  595\n",
      "  596  602  622  627  638  656  658  685  704  713  714  718  740  755\n",
      "  762  783  790  797  804  813  830  844  848  874  877  883  884  889\n",
      "  906  912  922  928  932  945  957  964  992  993  996 1006 1018 1019\n",
      " 1024 1038 1058 1060 1107 1128 1136 1137 1150 1186 1215 1216 1225 1286\n",
      " 1287 1312 1320 1321 1330 1335 1338 1350 1357 1390 1393 1396 1408 1450\n",
      " 1459 1471 1475 1490 1491 1543 1611 1612 1619 1626 1644 1653 1679 1681\n",
      " 1689 1704 1719 1727 1747 1755 1768 1791 1826 1832 1849 1851 1871 1882\n",
      " 1898 1902 1931 1946 1948 1954 1961 1995 2000 2003 2014 2029 2075 2095\n",
      " 2130 2135 2142 2149 2157 2160 2216 2238 2262 2272 2278 2279 2289 2341\n",
      " 2374 2380 2423 2432 2437 2452 2495 2505 2521 2545 2554 2572 2622 2624\n",
      " 2626 2645 2668 2685 2694 2702 2705 2767 2795 2796 2808 2817 2826 2838\n",
      " 2854 2870 2890 2922 2940 2946 2962 3001 3017 3034 3040 3070 3078 3087\n",
      " 3091 3095 3119 3120 3145 3151 3160 3165 3219 3234 3278 3302 3308 3310\n",
      " 3321 3322 3329 3340 3395 3405 3451 3456 3480 3481 3492 3493 3558 3580\n",
      " 3591 3607 3614 3617 3625 3637 3646 3668 3674 3679 3707 3737]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  30   43   48   61   62   63   79   92   99  119  128  131  133  163\n",
      "  205  206  244  252  260  310  356  364  379  382  395  399  455  470\n",
      "  471  475  495  501  508  523  555  560  569  582  597  598  611  633\n",
      "  643  697  698  720  724  730  733  739  741  752  769  810  815  826\n",
      "  846  856  870  938  939  953  981 1014 1041 1045 1048 1073 1132 1139\n",
      " 1140 1148 1151 1164 1169 1176 1185 1222 1224 1230 1256 1270 1274 1294\n",
      " 1295 1297 1304 1322 1325 1384 1387 1395 1424 1437 1455 1456 1470 1481\n",
      " 1519 1521 1522 1539 1551 1564 1574 1577 1589 1592 1595 1603 1604 1616\n",
      " 1625 1638 1651 1655 1671 1677 1725 1738 1748 1756 1759 1765 1777 1778\n",
      " 1797 1822 1827 1829 1830 1834 1835 1846 1850 1863 1897 1905 1907 1911\n",
      " 1951 1966 1967 1982 1988 2022 2025 2047 2058 2063 2073 2105 2134 2165\n",
      " 2202 2239 2244 2271 2283 2312 2318 2350 2353 2354 2373 2389 2395 2404\n",
      " 2419 2433 2453 2472 2478 2485 2498 2499 2500 2535 2563 2583 2590 2593\n",
      " 2597 2603 2608 2615 2616 2656 2658 2719 2736 2780 2818 2831 2913 2916\n",
      " 2923 2966 2999 3024 3041 3042 3072 3103 3130 3133 3158 3166 3181 3193\n",
      " 3205 3218 3237 3243 3249 3250 3287 3289 3298 3304 3323 3342 3372 3373\n",
      " 3374 3393 3406 3416 3424 3431 3457 3463 3494 3514 3525 3530 3536 3566\n",
      " 3592 3616 3631 3642 3653 3658 3692 3696 3709 3712 3718 3725]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  51   53   70  101  104  111  127  135  148  158  183  188  202  210\n",
      "  216  232  240  246  258  270  287  297  316  327  348  357  363  371\n",
      "  383  386  424  441  463  503  518  525  528  544  547  554  566  601\n",
      "  607  624  631  679  694  715  728  742  773  789  792  842  857  864\n",
      "  865  887  901  926  936  943  946  985 1001 1088 1099 1112 1117 1119\n",
      " 1147 1153 1157 1162 1184 1197 1203 1261 1269 1285 1313 1316 1331 1332\n",
      " 1368 1379 1398 1412 1414 1426 1436 1449 1452 1457 1472 1483 1488 1493\n",
      " 1500 1523 1548 1578 1608 1646 1667 1672 1673 1688 1693 1694 1716 1724\n",
      " 1762 1763 1770 1779 1807 1813 1817 1820 1821 1864 1875 1886 1890 1915\n",
      " 1921 1937 1939 1963 1976 1985 1998 2004 2007 2009 2039 2041 2046 2050\n",
      " 2065 2068 2076 2078 2094 2098 2111 2117 2143 2146 2170 2172 2177 2184\n",
      " 2213 2229 2242 2250 2268 2294 2296 2334 2336 2391 2402 2412 2438 2458\n",
      " 2466 2482 2484 2511 2531 2538 2557 2575 2595 2610 2611 2613 2619 2638\n",
      " 2646 2675 2695 2696 2716 2730 2752 2755 2799 2810 2815 2825 2840 2849\n",
      " 2853 2860 2867 2872 2883 2893 2898 2948 2954 2964 2967 2975 2977 2984\n",
      " 2991 3086 3097 3105 3173 3195 3198 3204 3213 3224 3227 3229 3230 3296\n",
      " 3318 3325 3350 3357 3365 3375 3390 3471 3495 3526 3531 3532 3538 3543\n",
      " 3545 3551 3565 3626 3629 3641 3666 3670 3690 3700 3711 3728]\n",
      "TRAIN: [   0    1    2 ... 3744 3745 3746] TEST: [   3   42   52   74   94  100  122  124  142  150  168  169  173  191\n",
      "  193  204  211  248  271  276  295  300  306  308  321  336  351  361\n",
      "  409  423  428  439  452  480  492  497  500  515  516  524  548  557\n",
      "  571  614  617  692  699  753  771  778  780  784  817  839  845  853\n",
      "  867  931  948  958  965  966  972  984  989 1013 1028 1062 1078 1096\n",
      " 1103 1111 1168 1191 1196 1209 1210 1219 1231 1268 1284 1309 1323 1399\n",
      " 1443 1463 1497 1498 1507 1511 1520 1547 1549 1554 1562 1565 1568 1602\n",
      " 1622 1640 1647 1660 1661 1663 1684 1691 1701 1712 1723 1742 1749 1752\n",
      " 1753 1764 1794 1803 1836 1847 1854 1899 1942 1944 1947 1950 1953 1957\n",
      " 1968 1969 1970 1975 1980 1987 1989 1994 2002 2018 2053 2062 2072 2074\n",
      " 2083 2093 2104 2138 2155 2176 2179 2252 2275 2291 2303 2304 2314 2329\n",
      " 2339 2357 2398 2414 2429 2456 2467 2468 2480 2486 2490 2512 2527 2569\n",
      " 2614 2618 2643 2650 2657 2681 2689 2712 2717 2721 2757 2759 2786 2793\n",
      " 2812 2845 2862 2875 2879 2886 2891 2894 2908 2955 2957 2958 2976 2995\n",
      " 3003 3020 3026 3027 3055 3066 3153 3155 3167 3171 3174 3194 3197 3210\n",
      " 3222 3228 3246 3261 3305 3312 3331 3339 3343 3355 3361 3368 3380 3385\n",
      " 3413 3430 3434 3435 3436 3459 3461 3475 3491 3520 3529 3535 3555 3561\n",
      " 3576 3598 3634 3651 3694 3699 3701 3702 3726 3732 3738 3747]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  14   28   29   33   36   40   49   54  114  164  189  209  225  264\n",
      "  274  275  322  334  342  373  377  391  398  420  459  466  482  511\n",
      "  552  561  563  567  600  608  615  619  620  639  649  650  657  660\n",
      "  663  666  669  670  684  695  736  763  802  803  818  840  860  878\n",
      "  885  899  956  963 1042 1053 1065 1091 1154 1159 1208 1251 1265 1281\n",
      " 1291 1299 1303 1324 1333 1339 1340 1346 1347 1352 1360 1370 1375 1402\n",
      " 1403 1415 1421 1465 1469 1479 1486 1494 1496 1524 1540 1545 1555 1570\n",
      " 1576 1586 1613 1620 1627 1652 1682 1733 1785 1786 1788 1792 1819 1831\n",
      " 1833 1848 1877 1878 1903 1904 1910 1926 1927 1945 2013 2016 2028 2034\n",
      " 2035 2038 2040 2056 2059 2088 2090 2102 2112 2116 2139 2144 2148 2162\n",
      " 2195 2200 2209 2231 2247 2260 2276 2287 2288 2290 2297 2301 2349 2381\n",
      " 2394 2403 2405 2416 2426 2431 2441 2448 2465 2471 2489 2514 2542 2544\n",
      " 2549 2567 2581 2586 2596 2598 2620 2630 2648 2652 2659 2667 2676 2677\n",
      " 2684 2691 2704 2734 2739 2742 2746 2758 2766 2770 2787 2811 2827 2835\n",
      " 2858 2914 2924 2942 2951 2981 3007 3010 3011 3025 3036 3062 3080 3083\n",
      " 3085 3088 3178 3231 3242 3248 3252 3264 3268 3280 3291 3295 3333 3336\n",
      " 3352 3376 3387 3408 3420 3444 3453 3460 3464 3503 3515 3518 3541 3544\n",
      " 3550 3554 3583 3596 3604 3632 3648 3649 3656 3662 3703 3704]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [   9   73   78   84  103  126  136  140  178  229  250  281  301  302\n",
      "  313  328  332  337  352  366  370  388  390  444  457  476  491  494\n",
      "  510  519  536  556  558  562  572  599  623  625  628  647  654  661\n",
      "  665  686  689  709  734  746  754  758  759  775  777  794  809  823\n",
      "  872  880  882  886  904  915  919  923  934  942  971  974  982  987\n",
      "  988 1008 1012 1015 1043 1047 1082 1106 1114 1127 1130 1134 1170 1173\n",
      " 1174 1177 1180 1218 1223 1233 1236 1242 1262 1263 1276 1280 1336 1344\n",
      " 1348 1349 1361 1364 1411 1487 1495 1533 1546 1581 1605 1630 1633 1634\n",
      " 1635 1711 1714 1741 1761 1796 1809 1839 1865 1879 1919 1925 1984 1996\n",
      " 2015 2061 2067 2085 2097 2113 2127 2150 2151 2156 2167 2205 2206 2240\n",
      " 2245 2261 2265 2299 2300 2309 2317 2325 2328 2331 2337 2351 2372 2376\n",
      " 2388 2422 2457 2474 2488 2517 2528 2546 2561 2562 2585 2588 2589 2625\n",
      " 2631 2639 2641 2660 2663 2674 2688 2703 2725 2729 2741 2753 2756 2783\n",
      " 2790 2807 2809 2832 2844 2846 2857 2861 2880 2900 2903 2935 2959 2969\n",
      " 2989 3032 3035 3049 3053 3057 3069 3071 3079 3082 3096 3112 3113 3132\n",
      " 3139 3142 3156 3164 3190 3211 3241 3255 3258 3271 3274 3276 3319 3330\n",
      " 3334 3337 3341 3354 3366 3438 3439 3474 3482 3486 3496 3539 3549 3562\n",
      " 3581 3594 3597 3612 3619 3652 3659 3671 3676 3681 3691 3735]\n",
      "TRAIN: [   0    1    2 ... 3743 3745 3747] TEST: [   4   15   66  115  157  160  172  174  187  208  217  219  238  268\n",
      "  269  272  278  285  298  304  324  344  394  397  403  405  445  447\n",
      "  462  474  487  493  499  505  521  529  540  543  570  574  584  585\n",
      "  588  589  609  613  621  653  682  690  702  706  710  711  712  717\n",
      "  723  732  749  751  760  785  806  855  859  881  888  890  930  954\n",
      "  980  986 1007 1033 1104 1113 1121 1146 1172 1187 1193 1221 1237 1249\n",
      " 1254 1273 1290 1292 1318 1319 1341 1358 1366 1405 1407 1409 1438 1477\n",
      " 1485 1489 1505 1515 1516 1535 1553 1561 1567 1606 1643 1696 1702 1713\n",
      " 1720 1728 1730 1735 1751 1760 1773 1814 1842 1853 1856 1908 1974 1981\n",
      " 1990 1991 2012 2030 2086 2087 2101 2119 2154 2183 2188 2192 2222 2228\n",
      " 2249 2254 2282 2292 2306 2319 2333 2343 2355 2364 2386 2392 2400 2449\n",
      " 2462 2477 2492 2524 2529 2534 2536 2555 2577 2580 2602 2623 2627 2628\n",
      " 2678 2686 2697 2701 2749 2771 2778 2785 2820 2821 2822 2830 2834 2839\n",
      " 2877 2889 2907 2909 2937 2941 2943 2947 2960 2970 2971 2973 2978 2980\n",
      " 2990 3031 3038 3044 3054 3056 3067 3074 3081 3099 3138 3143 3162 3175\n",
      " 3189 3217 3225 3233 3235 3251 3257 3259 3269 3270 3306 3309 3338 3382\n",
      " 3398 3417 3419 3445 3454 3466 3472 3476 3487 3489 3490 3507 3524 3534\n",
      " 3578 3593 3595 3667 3680 3715 3719 3727 3729 3733 3744 3746]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [   7   10   16   41   44   46   59   93  102  125  165  175  201  213\n",
      "  235  242  280  284  290  325  358  384  392  416  432  448  477  485\n",
      "  488  513  527  534  539  575  577  579  590  629  634  652  672  727\n",
      "  738  756  787  798  852  858  862  871  892  903  911  935  940  991\n",
      "  998 1021 1031 1032 1035 1039 1052 1063 1068 1076 1141 1179 1181 1227\n",
      " 1234 1240 1245 1293 1311 1342 1374 1377 1392 1413 1432 1440 1460 1501\n",
      " 1518 1525 1528 1534 1537 1572 1583 1594 1600 1610 1615 1617 1658 1659\n",
      " 1697 1700 1706 1709 1718 1726 1731 1739 1743 1746 1771 1782 1784 1799\n",
      " 1802 1838 1866 1880 1892 1896 1901 1922 1928 1933 1934 1971 1977 1978\n",
      " 1979 2005 2026 2027 2100 2159 2163 2171 2180 2185 2211 2214 2226 2234\n",
      " 2255 2257 2280 2293 2307 2311 2330 2344 2356 2406 2415 2417 2461 2481\n",
      " 2487 2497 2530 2532 2537 2573 2604 2640 2664 2683 2724 2727 2750 2769\n",
      " 2802 2823 2850 2856 2873 2874 2876 2896 2899 2904 2915 2925 2926 2930\n",
      " 2933 2938 2949 2968 2979 2987 3004 3005 3022 3048 3090 3092 3094 3098\n",
      " 3100 3110 3114 3123 3124 3129 3148 3161 3170 3172 3176 3183 3185 3191\n",
      " 3207 3226 3240 3263 3267 3283 3293 3297 3299 3311 3316 3320 3327 3335\n",
      " 3347 3364 3425 3449 3452 3458 3465 3479 3483 3485 3509 3528 3547 3570\n",
      " 3574 3586 3603 3610 3615 3622 3638 3650 3689 3698 3708 3713]\n",
      "TRAIN: [   0    2    3 ... 3745 3746 3747] TEST: [   1    6    8   20   27   34   39   47   83  121  123  139  155  177\n",
      "  226  234  236  249  253  265  283  296  305  307  335  365  372  385\n",
      "  387  396  404  425  440  484  486  489  496  507  532  549  632  636\n",
      "  637  648  664  677  725  744  747  765  782  793  808  824  828  851\n",
      "  861  863  868  875  914  927  937  952  967  968  983  999 1005 1009\n",
      " 1030 1051 1057 1067 1070 1087 1109 1129 1133 1165 1178 1188 1192 1211\n",
      " 1213 1214 1232 1253 1258 1264 1271 1282 1302 1305 1315 1345 1372 1385\n",
      " 1389 1400 1417 1420 1451 1480 1484 1506 1512 1513 1517 1550 1573 1629\n",
      " 1657 1662 1675 1686 1736 1737 1783 1790 1800 1810 1824 1840 1841 1843\n",
      " 1844 1855 1867 1869 1870 1872 1873 1888 1893 1913 1930 1956 1959 1972\n",
      " 2011 2032 2052 2060 2081 2089 2099 2107 2110 2124 2140 2152 2175 2178\n",
      " 2196 2198 2235 2241 2248 2253 2258 2267 2298 2324 2345 2358 2360 2361\n",
      " 2383 2397 2410 2501 2504 2516 2550 2558 2576 2584 2592 2666 2672 2673\n",
      " 2679 2687 2692 2693 2709 2723 2728 2735 2737 2743 2751 2772 2781 2782\n",
      " 2791 2794 2798 2805 2841 2855 2871 2952 2963 2972 2988 3012 3014 3019\n",
      " 3073 3109 3117 3147 3152 3192 3238 3256 3275 3282 3301 3307 3353 3356\n",
      " 3389 3391 3402 3404 3410 3411 3423 3447 3497 3512 3521 3542 3556 3573\n",
      " 3582 3599 3633 3661 3665 3672 3686 3693 3695 3731 3741 3742]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  19   32   71   76   81  120  141  144  149  166  167  171  186  262\n",
      "  279  282  299  303  338  343  345  354  362  407  426  430  434  436\n",
      "  451  478  479  490  498  502  504  517  537  565  573  578  586  644\n",
      "  646  651  659  662  667  668  673  676  688  693  703  743  757  761\n",
      "  781  796  814  905  920  925  955  959  961  970 1000 1002 1003 1022\n",
      " 1066 1072 1077 1080 1083 1086 1101 1115 1125 1135 1145 1182 1183 1195\n",
      " 1198 1200 1202 1205 1212 1235 1267 1279 1288 1300 1314 1381 1419 1423\n",
      " 1431 1433 1434 1439 1467 1482 1499 1526 1541 1542 1552 1557 1558 1569\n",
      " 1609 1648 1656 1685 1721 1740 1750 1767 1815 1828 1837 1859 1883 1900\n",
      " 1914 1924 1929 1932 1938 1940 1943 1958 2010 2019 2024 2031 2033 2064\n",
      " 2109 2115 2118 2121 2126 2168 2174 2199 2207 2210 2215 2220 2221 2232\n",
      " 2243 2259 2284 2285 2286 2316 2367 2401 2409 2411 2418 2424 2435 2439\n",
      " 2470 2507 2515 2523 2548 2571 2609 2612 2634 2655 2665 2669 2680 2699\n",
      " 2710 2715 2722 2738 2760 2761 2763 2777 2784 2801 2806 2865 2884 2897\n",
      " 2911 2918 2931 2983 2998 3029 3045 3089 3093 3121 3127 3144 3157 3168\n",
      " 3188 3196 3199 3215 3247 3260 3262 3265 3266 3300 3326 3360 3363 3383\n",
      " 3384 3414 3422 3426 3455 3468 3469 3478 3504 3513 3527 3537 3560 3568\n",
      " 3577 3585 3600 3602 3613 3645 3660 3673 3687 3724 3739 3743]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  22   31   58   65   77   89   97  116  130  153  154  161  196  224\n",
      "  230  245  286  288  289  319  320  331  333  341  367  374  400  418\n",
      "  431  467  473  481  541  568  580  592  594  603  605  635  640  678\n",
      "  683  691  700  748  831  841  850  895  909  929  951  960  969  997\n",
      " 1004 1011 1016 1020 1023 1026 1036 1037 1044 1061 1074 1079 1090 1095\n",
      " 1097 1098 1100 1110 1118 1124 1142 1175 1189 1201 1241 1246 1252 1260\n",
      " 1266 1277 1283 1306 1307 1308 1354 1355 1397 1401 1406 1416 1458 1462\n",
      " 1478 1502 1509 1510 1514 1527 1529 1531 1538 1585 1588 1593 1598 1614\n",
      " 1621 1637 1641 1664 1669 1676 1699 1708 1710 1717 1734 1745 1757 1775\n",
      " 1776 1781 1818 1887 1889 1894 1912 1949 1960 1986 2021 2043 2044 2045\n",
      " 2082 2084 2114 2131 2136 2164 2173 2181 2189 2191 2201 2227 2237 2251\n",
      " 2295 2313 2315 2323 2346 2352 2365 2378 2382 2384 2420 2450 2464 2469\n",
      " 2491 2509 2513 2519 2526 2552 2556 2560 2564 2601 2605 2606 2607 2636\n",
      " 2661 2708 2731 2732 2745 2773 2775 2792 2804 2828 2848 2866 2869 2878\n",
      " 2892 2950 2992 3023 3037 3059 3060 3061 3077 3084 3104 3116 3137 3140\n",
      " 3150 3179 3180 3187 3203 3220 3221 3245 3286 3315 3348 3358 3362 3392\n",
      " 3394 3400 3407 3412 3418 3432 3442 3473 3499 3506 3511 3517 3533 3540\n",
      " 3546 3559 3628 3640 3663 3664 3683 3685 3714 3721 3723 3734]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  11   18   23   50   55   56   68   85   98  110  112  137  146  156\n",
      "  222  223  228  251  256  309  317  359  376  419  429  435  506  531\n",
      "  576  612  641  642  680  716  735  750  807  812  822  833  849  873\n",
      "  913  918  921  962  975  977  994  995 1025 1029 1034 1040 1046 1055\n",
      " 1071 1085 1093 1094 1122 1138 1149 1156 1158 1166 1171 1194 1199 1204\n",
      " 1220 1248 1272 1296 1310 1343 1351 1353 1356 1363 1365 1371 1376 1378\n",
      " 1404 1422 1425 1435 1441 1442 1444 1445 1446 1448 1453 1492 1532 1579\n",
      " 1582 1584 1587 1597 1601 1618 1654 1665 1668 1678 1680 1692 1715 1744\n",
      " 1758 1766 1772 1787 1789 1801 1823 1852 1858 1874 1884 1895 1909 1917\n",
      " 1962 1964 1993 2023 2049 2051 2066 2070 2079 2080 2091 2120 2128 2137\n",
      " 2158 2187 2190 2223 2263 2269 2277 2320 2321 2347 2348 2370 2371 2396\n",
      " 2408 2413 2421 2428 2430 2445 2447 2473 2502 2506 2508 2520 2568 2570\n",
      " 2578 2587 2635 2671 2713 2718 2720 2733 2748 2762 2765 2774 2797 2800\n",
      " 2819 2833 2836 2837 2852 2859 2864 2881 2906 2917 2919 2932 2934 2953\n",
      " 2982 2993 3000 3009 3015 3021 3043 3046 3063 3064 3065 3075 3102 3122\n",
      " 3128 3149 3159 3184 3206 3216 3253 3272 3273 3277 3290 3345 3349 3367\n",
      " 3381 3396 3421 3428 3433 3441 3467 3477 3484 3501 3502 3505 3563 3564\n",
      " 3569 3572 3579 3623 3643 3647 3669 3677 3682 3705 3710]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [   5   17   57   80   87  107  129  134  143  151  152  162  181  182\n",
      "  185  190  194  197  203  214  218  243  254  261  267  277  292  293\n",
      "  323  369  375  380  401  408  411  413  417  421  422  443  449  509\n",
      "  512  535  546  564  583  604  610  630  655  696  701  707  708  729\n",
      "  731  745  764  770  774  788  801  820  821  834  838  869  876  907\n",
      "  910  917  941  944  976 1049 1064 1081 1089 1102 1116 1123 1144 1155\n",
      " 1167 1190 1229 1238 1243 1289 1328 1334 1380 1382 1394 1410 1429 1454\n",
      " 1461 1473 1476 1504 1530 1556 1559 1571 1580 1596 1607 1623 1636 1639\n",
      " 1666 1695 1698 1705 1754 1769 1780 1793 1798 1804 1805 1816 1825 1860\n",
      " 1861 1868 1965 1973 1992 1997 2006 2008 2048 2077 2106 2141 2145 2161\n",
      " 2166 2182 2208 2217 2218 2225 2230 2246 2270 2274 2302 2308 2310 2326\n",
      " 2332 2335 2340 2342 2368 2369 2385 2387 2407 2442 2444 2451 2463 2493\n",
      " 2518 2539 2540 2541 2543 2565 2574 2582 2599 2629 2633 2637 2653 2682\n",
      " 2714 2747 2768 2776 2788 2814 2816 2842 2843 2847 2868 2885 2895 2901\n",
      " 2927 2929 2936 2945 2956 2974 3013 3033 3039 3047 3051 3058 3125 3126\n",
      " 3186 3209 3212 3239 3285 3292 3294 3324 3332 3344 3346 3351 3359 3370\n",
      " 3371 3378 3386 3397 3427 3429 3450 3516 3523 3548 3571 3584 3588 3618\n",
      " 3624 3627 3636 3639 3655 3675 3684 3688 3706 3717 3736]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [   6   10   33   37   80   83   87   94  100  154  157  165  175  207\n",
      "  232  242  271  296  298  306  311  327  330  337  347  366  378  399\n",
      "  405  416  417  441  469  474  479  485  517  526  538  544  554  559\n",
      "  560  571  586  592  599  606  622  630  635  638  674  676  679  700\n",
      "  709  724  730  745  754  774  783  812  834  857  864  868  873  905\n",
      "  920  960  967  982 1001 1009 1025 1044 1097 1100 1116 1138 1147 1173\n",
      " 1213 1215 1247 1260 1272 1274 1278 1290 1309 1310 1316 1323 1328 1336\n",
      " 1352 1389 1403 1471 1480 1481 1508 1509 1535 1591 1592 1596 1599 1611\n",
      " 1621 1623 1632 1658 1681 1691 1706 1725 1769 1822 1839 1846 1863 1873\n",
      " 1883 1890 1901 1928 1973 1975 1977 1983 2011 2013 2044 2050 2053 2056\n",
      " 2082 2109 2114 2115 2132 2147 2161 2162 2215 2223 2253 2277 2280 2320\n",
      " 2324 2340 2407 2411 2413 2441 2461 2487 2506 2507 2540 2546 2554 2561\n",
      " 2568 2575 2588 2599 2642 2651 2654 2657 2679 2685 2689 2703 2714 2719\n",
      " 2735 2746 2756 2783 2785 2787 2797 2813 2834 2857 2884 2895 2930 2949\n",
      " 2952 2959 2961 2985 2994 3003 3013 3015 3064 3084 3095 3107 3115 3131\n",
      " 3147 3151 3165 3183 3187 3193 3203 3204 3218 3231 3250 3256 3284 3302\n",
      " 3312 3313 3322 3351 3362 3368 3418 3429 3490 3502 3509 3515 3536 3545\n",
      " 3578 3581 3583 3584 3626 3636 3642 3670 3682 3705 3735 3740]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  17   25   35   39   42   76   90  101  113  135  147  150  162  164\n",
      "  167  172  182  189  195  208  210  231  235  244  252  265  272  277\n",
      "  290  303  307  317  323  325  333  361  381  409  428  431  433  449\n",
      "  502  504  561  578  616  646  683  699  710  720  750  761  776  790\n",
      "  814  830  872  876  889  930  935 1015 1031 1062 1078 1104 1129 1182\n",
      " 1212 1220 1221 1232 1246 1258 1261 1262 1265 1270 1277 1308 1313 1343\n",
      " 1347 1368 1374 1397 1412 1449 1459 1517 1519 1539 1565 1568 1585 1616\n",
      " 1624 1633 1637 1641 1642 1674 1689 1707 1744 1758 1761 1779 1781 1782\n",
      " 1793 1798 1805 1844 1852 1857 1860 1867 1869 1870 1892 1897 1905 1918\n",
      " 1932 1960 1962 1976 2043 2046 2059 2068 2089 2101 2106 2108 2110 2143\n",
      " 2146 2166 2167 2190 2192 2199 2221 2232 2245 2249 2261 2266 2270 2276\n",
      " 2294 2402 2430 2436 2477 2495 2503 2543 2553 2565 2586 2605 2647 2670\n",
      " 2675 2681 2691 2695 2696 2700 2712 2730 2763 2769 2773 2780 2814 2833\n",
      " 2836 2845 2850 2853 2854 2863 2868 2903 2935 2947 2962 2968 2987 2990\n",
      " 3001 3022 3038 3048 3096 3144 3155 3167 3169 3170 3199 3210 3219 3234\n",
      " 3244 3252 3265 3282 3296 3311 3323 3329 3353 3361 3363 3373 3382 3387\n",
      " 3398 3420 3436 3444 3452 3455 3466 3473 3487 3496 3552 3558 3587 3596\n",
      " 3603 3611 3624 3640 3654 3658 3665 3687 3693 3695 3720 3741]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  21   28   61   64   91   97  112  138  141  149  192  198  211  223\n",
      "  230  233  236  237  285  301  309  329  350  355  371  377  394  404\n",
      "  415  419  436  440  481  507  519  552  598  601  615  643  652  659\n",
      "  664  704  705  714  719  731  769  797  806  819  839  848  853  860\n",
      "  869  890  892  918  942  951  968  975  992 1000 1011 1018 1033 1086\n",
      " 1109 1115 1163 1175 1178 1184 1189 1190 1198 1210 1214 1237 1273 1282\n",
      " 1286 1293 1312 1330 1333 1353 1375 1380 1384 1417 1422 1425 1428 1441\n",
      " 1458 1475 1503 1524 1531 1536 1537 1544 1559 1594 1634 1646 1675 1677\n",
      " 1713 1720 1726 1764 1771 1812 1817 1821 1829 1847 1886 1887 1909 1912\n",
      " 1942 1945 1987 2018 2020 2024 2072 2084 2086 2088 2094 2098 2118 2128\n",
      " 2136 2157 2158 2176 2185 2196 2210 2219 2226 2233 2239 2242 2246 2282\n",
      " 2303 2313 2321 2374 2404 2408 2417 2443 2469 2481 2502 2504 2505 2515\n",
      " 2539 2579 2582 2598 2618 2619 2623 2627 2630 2631 2652 2655 2664 2682\n",
      " 2698 2717 2726 2732 2765 2779 2810 2842 2864 2867 2883 2886 2909 2912\n",
      " 2919 2920 2926 2936 2946 2979 3011 3017 3021 3049 3052 3072 3083 3089\n",
      " 3112 3127 3142 3174 3184 3194 3217 3237 3245 3253 3298 3308 3358 3360\n",
      " 3365 3379 3405 3417 3459 3461 3467 3475 3482 3486 3495 3531 3559 3562\n",
      " 3569 3588 3606 3618 3629 3634 3647 3663 3699 3710 3715 3731]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  47   55   67   68   82   88   89  109  123  139  170  196  205  215\n",
      "  222  246  259  305  314  322  334  343  367  370  373  443  445  457\n",
      "  458  470  480  498  513  539  542  546  551  563  564  568  582  591\n",
      "  612  647  669  702  717  743  771  782  861  887  896  898  906  916\n",
      "  971 1002 1010 1021 1023 1030 1036 1040 1043 1061 1073 1076 1082 1106\n",
      " 1128 1131 1135 1157 1159 1165 1166 1224 1226 1250 1252 1254 1306 1340\n",
      " 1350 1367 1394 1398 1420 1423 1429 1433 1452 1453 1467 1468 1479 1516\n",
      " 1526 1543 1548 1549 1553 1554 1573 1610 1627 1649 1668 1698 1704 1710\n",
      " 1762 1808 1811 1825 1831 1832 1853 1878 1893 1899 1906 1933 1935 1936\n",
      " 1944 1954 1958 2009 2017 2030 2034 2037 2038 2052 2061 2064 2066 2073\n",
      " 2085 2095 2100 2105 2113 2122 2173 2193 2197 2218 2255 2264 2299 2304\n",
      " 2308 2322 2325 2337 2372 2373 2386 2391 2416 2425 2433 2437 2458 2459\n",
      " 2460 2467 2533 2548 2574 2576 2587 2601 2604 2629 2649 2658 2660 2671\n",
      " 2687 2697 2707 2710 2752 2840 2859 2892 2922 2940 2943 2950 2957 2980\n",
      " 3031 3044 3056 3069 3081 3099 3121 3135 3160 3168 3179 3188 3249 3275\n",
      " 3314 3321 3325 3330 3344 3352 3376 3388 3393 3396 3403 3404 3424 3433\n",
      " 3434 3442 3445 3449 3471 3474 3477 3479 3483 3488 3497 3498 3504 3511\n",
      " 3556 3563 3614 3619 3632 3669 3678 3698 3713 3723 3737 3738]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [   5    9   13   27   34   38   51   57   95   98   99  119  136  177\n",
      "  194  214  238  240  254  257  268  331  341  348  374  390  406  435\n",
      "  493  509  529  532  536  547  583  593  658  665  751  752  757  766\n",
      "  778  786  791  822  824  826  831  897  899  917  922  925  934  939\n",
      "  961  970  983  985  986 1007 1012 1027 1066 1074 1075 1099 1103 1107\n",
      " 1117 1130 1132 1142 1148 1152 1187 1188 1204 1238 1275 1285 1289 1298\n",
      " 1334 1344 1359 1382 1390 1393 1421 1444 1460 1463 1476 1482 1486 1497\n",
      " 1515 1538 1588 1600 1603 1644 1678 1695 1703 1705 1719 1721 1731 1734\n",
      " 1747 1754 1756 1766 1785 1795 1818 1819 1851 1856 1876 1888 1889 1913\n",
      " 1916 1937 1986 1992 2006 2021 2042 2117 2127 2149 2151 2169 2183 2189\n",
      " 2205 2257 2268 2271 2274 2291 2305 2319 2323 2341 2353 2368 2396 2401\n",
      " 2412 2420 2448 2451 2456 2457 2483 2501 2551 2591 2602 2612 2613 2636\n",
      " 2694 2699 2702 2724 2737 2738 2759 2766 2772 2789 2791 2792 2807 2825\n",
      " 2832 2841 2846 2856 2866 2880 2890 2905 2908 2921 2953 2984 2997 3020\n",
      " 3029 3033 3041 3047 3051 3055 3057 3060 3067 3073 3097 3113 3128 3134\n",
      " 3139 3143 3172 3189 3206 3223 3224 3229 3242 3257 3271 3290 3295 3300\n",
      " 3317 3391 3394 3413 3447 3450 3468 3472 3520 3534 3544 3570 3608 3609\n",
      " 3625 3643 3648 3653 3660 3664 3707 3714 3721 3725 3739 3744]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  18   19   31   84  110  156  160  184  188  190  212  217  227  234\n",
      "  292  339  345  351  359  386  420  434  438  442  463  491  496  503\n",
      "  516  518  528  541  548  562  570  587  597  614  626  629  631  633\n",
      "  685  698  712  715  721  727  728  733  734  736  737  744  789  803\n",
      "  817  838  877  880  907  911  923  928  945  952  954  989  991 1041\n",
      " 1042 1070 1071 1081 1121 1145 1158 1174 1192 1197 1206 1211 1222 1244\n",
      " 1245 1253 1271 1287 1301 1348 1360 1361 1366 1371 1377 1392 1395 1405\n",
      " 1426 1435 1436 1448 1462 1478 1490 1498 1528 1540 1541 1561 1564 1635\n",
      " 1648 1657 1660 1670 1679 1686 1694 1715 1735 1736 1749 1751 1763 1776\n",
      " 1784 1786 1789 1799 1810 1824 1841 1855 1938 1970 1974 1985 1995 1997\n",
      " 2014 2039 2071 2093 2102 2119 2137 2150 2152 2156 2178 2194 2201 2209\n",
      " 2231 2273 2275 2283 2296 2327 2346 2349 2354 2355 2393 2440 2444 2449\n",
      " 2450 2479 2528 2535 2547 2566 2606 2615 2628 2638 2648 2650 2661 2665\n",
      " 2684 2686 2705 2706 2734 2818 2820 2837 2851 2879 2881 2888 2901 2923\n",
      " 2924 2934 2970 2991 2999 3004 3009 3046 3054 3074 3080 3086 3120 3122\n",
      " 3136 3141 3145 3150 3158 3182 3192 3214 3222 3236 3241 3262 3281 3304\n",
      " 3315 3412 3419 3427 3438 3448 3470 3476 3494 3514 3526 3539 3567 3571\n",
      " 3577 3582 3604 3621 3623 3628 3680 3681 3700 3716 3727 3734]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [   8   12   66   69   74   77   85  128  134  142  169  171  173  183\n",
      "  200  201  209  263  275  278  284  289  294  310  321  338  368  372\n",
      "  382  395  403  412  421  447  459  461  473  501  531  540  576  600\n",
      "  604  610  617  640  641  642  667  677  688  690  695  758  770  775\n",
      "  820  833  835  843  859  895  900  901  913  919  921  924  926  962\n",
      "  974  984 1022 1065 1068 1072 1079 1108 1112 1114 1122 1124 1126 1139\n",
      " 1146 1183 1209 1216 1231 1235 1248 1257 1264 1269 1281 1284 1292 1369\n",
      " 1372 1381 1413 1416 1427 1437 1445 1461 1464 1466 1484 1485 1488 1518\n",
      " 1521 1550 1571 1587 1589 1593 1597 1604 1640 1650 1665 1727 1730 1741\n",
      " 1748 1770 1775 1783 1794 1801 1802 1858 1882 1908 1934 1957 1965 2001\n",
      " 2008 2010 2022 2027 2049 2051 2062 2074 2090 2111 2123 2144 2188 2227\n",
      " 2237 2243 2258 2289 2311 2332 2345 2356 2363 2364 2366 2400 2428 2439\n",
      " 2470 2475 2486 2489 2513 2518 2519 2531 2538 2555 2557 2564 2572 2578\n",
      " 2581 2603 2609 2634 2715 2721 2728 2736 2754 2757 2771 2801 2824 2830\n",
      " 2861 2865 2876 2896 2939 2955 2971 2973 2988 3014 3036 3061 3065 3071\n",
      " 3078 3088 3093 3100 3118 3123 3176 3178 3259 3260 3261 3277 3301 3303\n",
      " 3309 3324 3336 3364 3385 3408 3409 3465 3491 3551 3561 3564 3579 3601\n",
      " 3616 3630 3631 3644 3659 3674 3691 3702 3703 3711 3719 3726]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  23   24   46   70   71   93  106  126  140  158  163  178  179  191\n",
      "  199  220  226  228  243  249  253  266  283  304  336  363  365  383\n",
      "  396  425  427  456  476  514  535  550  557  569  579  580  588  589\n",
      "  605  623  634  650  668  671  680  681  703  708  753  788  804  813\n",
      "  815  846  854  855  871  885  886  894  904  948  957  977  990  999\n",
      " 1054 1057 1077 1083 1088 1095 1101 1120 1127 1149 1161 1164 1171 1180\n",
      " 1196 1228 1236 1239 1256 1280 1302 1303 1305 1322 1325 1327 1345 1355\n",
      " 1365 1391 1406 1419 1431 1451 1456 1457 1470 1477 1483 1493 1494 1505\n",
      " 1513 1555 1574 1582 1601 1613 1643 1651 1653 1680 1688 1700 1750 1797\n",
      " 1807 1809 1828 1836 1861 1879 1885 1891 1904 1924 1940 1947 1951 1961\n",
      " 1988 1996 1998 1999 2004 2012 2035 2040 2054 2058 2079 2120 2134 2145\n",
      " 2159 2160 2181 2182 2208 2228 2248 2251 2263 2292 2306 2315 2328 2351\n",
      " 2358 2359 2378 2381 2383 2424 2446 2473 2474 2490 2496 2499 2510 2522\n",
      " 2585 2592 2595 2692 2704 2711 2741 2747 2762 2788 2798 2826 2897 2899\n",
      " 2917 2928 2951 2965 2972 2992 2998 3032 3043 3045 3075 3076 3106 3111\n",
      " 3116 3117 3119 3129 3148 3181 3198 3230 3238 3266 3274 3278 3299 3306\n",
      " 3343 3346 3348 3359 3370 3375 3410 3462 3532 3541 3557 3560 3566 3585\n",
      " 3590 3599 3637 3650 3673 3676 3686 3688 3694 3696 3701 3709]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  29   43   53   72  116  148  151  187  193  204  213  261  279  282\n",
      "  300  302  328  349  380  385  398  408  424  437  450  452  454  475\n",
      "  495  500  549  556  596  613  620  621  657  735  740  749  784  802\n",
      "  805  807  809  816  842  862  878  883  888  891  943  949  953  972\n",
      "  994 1006 1029 1039 1047 1059 1060 1067 1069 1084 1087 1113 1134 1150\n",
      " 1154 1169 1170 1202 1219 1227 1251 1283 1307 1321 1339 1342 1351 1379\n",
      " 1399 1401 1415 1440 1447 1487 1489 1580 1598 1607 1615 1617 1619 1663\n",
      " 1671 1687 1692 1724 1738 1743 1745 1759 1778 1820 1849 1854 1862 1871\n",
      " 1877 1884 1898 1902 1920 1925 1948 1968 1971 2026 2029 2032 2033 2048\n",
      " 2097 2104 2107 2112 2125 2129 2155 2180 2186 2191 2211 2212 2229 2240\n",
      " 2256 2278 2284 2286 2309 2312 2329 2335 2336 2344 2347 2362 2375 2390\n",
      " 2397 2415 2421 2429 2434 2435 2452 2465 2466 2476 2480 2492 2497 2517\n",
      " 2520 2563 2580 2600 2610 2632 2653 2673 2674 2680 2701 2722 2725 2729\n",
      " 2731 2733 2743 2755 2767 2806 2819 2822 2839 2843 2844 2849 2871 2873\n",
      " 2898 2904 2960 2975 2993 3005 3006 3012 3016 3028 3066 3079 3104 3109\n",
      " 3114 3149 3156 3186 3208 3220 3243 3264 3289 3310 3316 3318 3335 3338\n",
      " 3340 3345 3356 3371 3395 3414 3435 3463 3507 3525 3527 3546 3553 3565\n",
      " 3568 3580 3600 3627 3639 3683 3706 3718 3722 3730 3732 3742]\n",
      "TRAIN: [   1    2    4 ... 3745 3746 3747] TEST: [   0    3   11   16   78   79  125  137  145  152  168  174  202  203\n",
      "  229  256  273  276  288  295  324  326  340  353  358  375  397  422\n",
      "  448  460  464  466  490  527  530  543  572  594  607  611  649  654\n",
      "  694  697  722  725  732  748  772  781  799  801  818  825  827  836\n",
      "  845  852  863  879  881  912  931  933  941  966  973  980  995  998\n",
      " 1013 1046 1053 1085 1105 1137 1141 1172 1186 1194 1205 1234 1259 1294\n",
      " 1311 1315 1354 1358 1383 1400 1408 1465 1469 1500 1506 1520 1529 1532\n",
      " 1533 1546 1551 1609 1630 1662 1669 1672 1693 1697 1708 1722 1728 1732\n",
      " 1746 1768 1792 1804 1806 1823 1838 1864 1868 1875 1895 1896 1900 1903\n",
      " 1919 1921 1926 1930 1941 1966 2007 2065 2075 2091 2092 2099 2141 2207\n",
      " 2214 2225 2230 2235 2238 2244 2260 2262 2288 2298 2300 2307 2326 2338\n",
      " 2365 2388 2399 2410 2422 2431 2488 2494 2508 2512 2523 2529 2556 2584\n",
      " 2622 2666 2676 2709 2742 2745 2749 2777 2790 2793 2796 2799 2804 2812\n",
      " 2816 2821 2829 2852 2860 2869 2900 2910 2925 2941 2942 2944 2982 2996\n",
      " 3024 3035 3059 3062 3068 3087 3124 3140 3154 3157 3180 3185 3191 3197\n",
      " 3209 3246 3247 3248 3254 3258 3272 3285 3294 3327 3328 3334 3339 3341\n",
      " 3342 3355 3383 3422 3425 3432 3451 3456 3500 3501 3512 3524 3550 3594\n",
      " 3597 3610 3613 3635 3641 3652 3661 3671 3672 3692 3724 3736]\n",
      "TRAIN: [   0    1    3 ... 3744 3745 3746] TEST: [   2   14   22   40   45   52   81  117  120  122  127  131  143  146\n",
      "  159  239  250  251  297  313  315  320  344  354  360  393  411  414\n",
      "  444  462  468  483  515  521  525  533  545  565  574  575  595  602\n",
      "  608  609  625  632  673  684  687  689  693  706  707  729  762  763\n",
      "  765  768  773  821  823  837  844  866  914  937  959 1019 1032 1038\n",
      " 1048 1125 1144 1153 1181 1193 1199 1208 1242 1243 1288 1291 1296 1318\n",
      " 1320 1349 1364 1376 1402 1404 1430 1442 1491 1492 1511 1512 1522 1523\n",
      " 1562 1563 1569 1575 1578 1608 1628 1629 1631 1639 1659 1673 1690 1701\n",
      " 1716 1760 1777 1787 1790 1827 1830 1835 1850 1865 1917 1923 1931 1939\n",
      " 1952 1969 1972 1984 1989 1993 1994 2005 2023 2076 2078 2124 2126 2130\n",
      " 2148 2175 2200 2220 2241 2281 2287 2293 2317 2333 2342 2357 2369 2376\n",
      " 2379 2384 2387 2398 2409 2426 2463 2493 2498 2514 2516 2521 2527 2549\n",
      " 2573 2589 2590 2617 2635 2645 2659 2667 2669 2677 2693 2748 2778 2784\n",
      " 2795 2800 2803 2815 2847 2872 2874 2889 2915 2918 2954 2969 2976 2983\n",
      " 3010 3034 3050 3063 3070 3098 3101 3125 3138 3161 3162 3166 3190 3195\n",
      " 3202 3205 3207 3216 3228 3240 3263 3270 3305 3319 3332 3367 3372 3374\n",
      " 3378 3384 3400 3402 3423 3431 3439 3469 3481 3489 3519 3533 3535 3537\n",
      " 3540 3591 3612 3617 3633 3638 3646 3657 3666 3684 3729 3747]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ... 3744 3746 3747] TEST: [  26   41   56   58   62   73   86  108  115  118  124  129  144  161\n",
      "  181  185  197  206  224  280  281  332  357  362  364  384  392  401\n",
      "  407  471  484  506  520  522  558  567  577  581  585  619  627  637\n",
      "  651  653  701  716  718  739  755  756  764  767  780  793  795  811\n",
      "  847  849  858  874  882  902  903  915  932  950  955  965  979  981\n",
      "  988  997 1014 1017 1028 1056 1110 1118 1123 1136 1223 1230 1249 1263\n",
      " 1276 1279 1295 1300 1304 1331 1335 1407 1424 1438 1474 1502 1534 1558\n",
      " 1567 1572 1577 1579 1581 1584 1605 1612 1614 1636 1655 1661 1683 1717\n",
      " 1723 1752 1767 1772 1791 1803 1834 1845 1848 1866 1872 1894 1914 1922\n",
      " 1927 1953 1959 1980 1990 2002 2003 2036 2047 2055 2067 2077 2131 2133\n",
      " 2154 2168 2174 2198 2203 2206 2222 2254 2272 2301 2350 2352 2385 2394\n",
      " 2414 2445 2462 2468 2471 2509 2530 2534 2536 2542 2544 2571 2593 2594\n",
      " 2611 2626 2641 2744 2751 2758 2761 2805 2809 2811 2831 2838 2855 2858\n",
      " 2877 2882 2885 2887 2893 2906 2907 2938 2945 2966 2977 3007 3008 3018\n",
      " 3019 3025 3027 3042 3058 3092 3102 3103 3126 3152 3175 3200 3201 3221\n",
      " 3226 3227 3232 3239 3273 3288 3293 3307 3350 3369 3377 3380 3397 3399\n",
      " 3416 3421 3428 3453 3454 3458 3460 3464 3478 3493 3506 3522 3528 3542\n",
      " 3543 3547 3549 3620 3622 3675 3677 3690 3697 3717 3743 3745]\n",
      "TRAIN: [   0    1    2 ... 3744 3745 3747] TEST: [   4    7   15   20   30   32   36   44   48   54   60   63  107  111\n",
      "  153  166  216  255  262  264  267  291  299  318  369  379  430  453\n",
      "  467  482  489  492  494  499  508  510  523  524  553  566  573  644\n",
      "  661  662  670  672  675  686  696  711  726  741  742  798  832  840\n",
      "  841  850  884  929  940  944  946  947  958  976  993 1003 1004 1016\n",
      " 1026 1034 1049 1080 1090 1094 1102 1133 1140 1151 1155 1160 1168 1176\n",
      " 1179 1233 1267 1268 1297 1319 1324 1341 1356 1357 1378 1386 1396 1414\n",
      " 1432 1434 1439 1443 1454 1473 1507 1542 1547 1556 1566 1583 1590 1602\n",
      " 1618 1620 1625 1638 1667 1696 1718 1729 1739 1740 1742 1753 1757 1773\n",
      " 1780 1813 1815 1826 1833 1840 1842 1881 1943 1981 1991 2000 2015 2031\n",
      " 2045 2063 2080 2096 2138 2139 2140 2165 2170 2177 2216 2217 2259 2265\n",
      " 2302 2316 2318 2330 2361 2367 2370 2392 2405 2418 2419 2427 2442 2454\n",
      " 2464 2472 2478 2484 2491 2500 2532 2541 2545 2550 2558 2560 2562 2567\n",
      " 2570 2596 2607 2625 2633 2637 2640 2668 2672 2683 2723 2739 2753 2764\n",
      " 2774 2776 2786 2808 2848 2862 2894 2902 2914 2929 2931 2963 2978 2995\n",
      " 3002 3030 3039 3077 3082 3105 3132 3153 3164 3171 3196 3211 3279 3287\n",
      " 3320 3326 3331 3333 3357 3411 3426 3440 3484 3485 3492 3503 3505 3521\n",
      " 3538 3548 3575 3593 3615 3645 3662 3685 3708 3712 3728 3746]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  59   75   92  102  105  130  132  155  218  219  245  247  269  274\n",
      "  287  308  316  319  342  346  352  376  387  388  389  391  400  413\n",
      "  418  423  426  439  455  478  487  488  505  534  537  555  628  639\n",
      "  648  656  678  691  723  746  747  760  779  785  787  792  794  796\n",
      "  810  851  865  893  908  909  910  927  936  963  964  978 1005 1020\n",
      " 1037 1055 1089 1093 1098 1156 1162 1167 1177 1191 1203 1218 1225 1255\n",
      " 1299 1314 1317 1337 1346 1362 1363 1370 1373 1385 1410 1411 1418 1450\n",
      " 1501 1504 1510 1514 1525 1530 1552 1557 1586 1626 1652 1654 1656 1664\n",
      " 1666 1676 1682 1684 1699 1712 1714 1733 1737 1755 1788 1800 1814 1843\n",
      " 1874 1911 1929 1946 1949 1956 1963 1964 1978 1982 2028 2057 2069 2070\n",
      " 2103 2116 2135 2142 2179 2204 2213 2224 2236 2285 2310 2314 2334 2360\n",
      " 2380 2382 2389 2432 2438 2453 2455 2482 2511 2525 2537 2559 2577 2608\n",
      " 2639 2646 2656 2662 2663 2688 2690 2708 2716 2718 2727 2775 2781 2794\n",
      " 2802 2828 2835 2870 2875 2913 2933 2964 2967 2974 2981 2989 3026 3085\n",
      " 3090 3091 3110 3130 3133 3137 3146 3177 3212 3213 3225 3233 3267 3268\n",
      " 3269 3283 3286 3292 3297 3337 3354 3366 3381 3386 3389 3390 3392 3401\n",
      " 3406 3407 3415 3430 3441 3443 3446 3457 3480 3517 3529 3530 3554 3572\n",
      " 3573 3589 3598 3602 3607 3651 3655 3656 3679 3689 3704]\n",
      "TRAIN: [   0    2    3 ... 3745 3746 3747] TEST: [   1   49   50   65   96  103  104  114  121  133  176  180  186  221\n",
      "  225  241  248  258  260  270  286  293  312  335  356  402  410  429\n",
      "  432  446  451  465  472  477  486  497  511  512  584  590  603  618\n",
      "  624  636  645  655  660  663  666  682  692  713  738  759  777  800\n",
      "  808  828  829  856  867  870  875  938  956  969  987  996 1008 1024\n",
      " 1035 1045 1050 1051 1052 1058 1063 1064 1091 1092 1096 1111 1119 1143\n",
      " 1185 1195 1200 1201 1207 1217 1229 1240 1241 1266 1326 1329 1332 1338\n",
      " 1387 1388 1409 1446 1455 1472 1495 1496 1499 1527 1545 1560 1570 1576\n",
      " 1595 1606 1622 1645 1647 1685 1702 1709 1711 1765 1774 1796 1816 1837\n",
      " 1859 1880 1907 1910 1915 1950 1955 1967 1979 2016 2019 2025 2041 2060\n",
      " 2081 2083 2087 2121 2153 2163 2164 2171 2172 2184 2187 2195 2202 2234\n",
      " 2247 2250 2252 2267 2269 2279 2290 2295 2297 2331 2339 2343 2348 2371\n",
      " 2377 2395 2403 2406 2423 2447 2485 2524 2526 2552 2569 2583 2597 2614\n",
      " 2616 2620 2621 2624 2643 2644 2678 2713 2720 2740 2750 2760 2768 2770\n",
      " 2782 2817 2823 2827 2878 2891 2911 2916 2927 2932 2937 2948 2956 2958\n",
      " 2986 3000 3023 3037 3040 3053 3094 3108 3159 3163 3173 3215 3235 3251\n",
      " 3255 3276 3280 3291 3347 3349 3437 3499 3508 3510 3513 3516 3518 3523\n",
      " 3555 3574 3576 3586 3592 3595 3605 3649 3667 3668 3733]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  48   53   89   99  109  151  155  159  181  182  194  195  209  238\n",
      "  239  276  287  291  298  310  325  369  378  383  406  420  424  450\n",
      "  452  492  524  529  535  553  556  604  623  653  666  672  680  683\n",
      "  696  706  710  716  752  765  782  787  789  799  822  832  845  849\n",
      "  878  884  915  925  926  932  960  965  975  994 1000 1017 1018 1042\n",
      " 1056 1074 1098 1141 1165 1170 1173 1180 1183 1185 1189 1207 1229 1247\n",
      " 1250 1269 1295 1361 1363 1382 1383 1384 1423 1475 1508 1511 1515 1536\n",
      " 1540 1555 1568 1593 1622 1670 1687 1688 1695 1708 1717 1736 1753 1767\n",
      " 1781 1791 1793 1795 1801 1837 1848 1867 1876 1879 1888 1893 1894 1906\n",
      " 1920 1923 1946 1958 1964 1965 1996 2011 2045 2053 2065 2083 2124 2140\n",
      " 2158 2164 2187 2209 2215 2226 2234 2244 2263 2277 2284 2291 2295 2298\n",
      " 2391 2397 2412 2413 2448 2450 2463 2466 2469 2484 2487 2491 2513 2528\n",
      " 2558 2563 2568 2575 2587 2592 2603 2610 2612 2626 2634 2641 2660 2662\n",
      " 2664 2667 2681 2684 2703 2742 2750 2755 2771 2805 2837 2884 2901 2903\n",
      " 2941 2945 2962 2974 2977 3013 3028 3029 3038 3047 3090 3094 3122 3163\n",
      " 3188 3212 3229 3232 3250 3268 3274 3297 3300 3327 3333 3357 3361 3366\n",
      " 3368 3409 3421 3422 3437 3460 3464 3468 3471 3489 3516 3517 3524 3557\n",
      " 3561 3571 3592 3620 3637 3638 3659 3665 3678 3737 3738 3742]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  11   14   24   26   32   39   67   71   72   94  103  128  140  143\n",
      "  167  214  218  229  267  277  279  296  309  332  346  354  376  416\n",
      "  454  469  484  501  507  510  540  575  580  583  588  595  617  629\n",
      "  631  658  677  678  679  713  720  730  736  741  774  776  781  868\n",
      "  887  891  894  953  966  972  998 1026 1033 1050 1060 1075 1077 1079\n",
      " 1083 1097 1115 1121 1184 1200 1203 1243 1248 1274 1289 1293 1330 1357\n",
      " 1359 1362 1365 1367 1368 1373 1389 1396 1399 1447 1453 1457 1459 1467\n",
      " 1468 1491 1500 1533 1546 1564 1574 1580 1606 1618 1640 1672 1680 1684\n",
      " 1727 1730 1738 1764 1772 1782 1785 1809 1811 1821 1823 1825 1834 1847\n",
      " 1849 1852 1875 1892 1910 1912 1913 1990 2024 2035 2046 2052 2054 2067\n",
      " 2074 2077 2087 2109 2132 2135 2142 2168 2182 2183 2185 2210 2240 2241\n",
      " 2256 2264 2278 2310 2325 2346 2349 2389 2400 2444 2445 2462 2506 2537\n",
      " 2542 2548 2556 2560 2565 2572 2618 2630 2648 2701 2716 2717 2736 2743\n",
      " 2762 2775 2777 2797 2806 2818 2826 2848 2857 2866 2886 2891 2898 2936\n",
      " 2937 2951 2956 2966 2997 3001 3069 3086 3095 3098 3144 3179 3201 3204\n",
      " 3208 3211 3215 3241 3252 3260 3306 3308 3311 3314 3340 3349 3381 3383\n",
      " 3399 3400 3420 3429 3430 3433 3443 3451 3480 3491 3494 3495 3535 3547\n",
      " 3560 3565 3587 3588 3639 3651 3682 3702 3710 3726 3735 3744]\n",
      "TRAIN: [   1    2    3 ... 3745 3746 3747] TEST: [   0   49   64   82  105  110  115  117  120  130  131  144  146  153\n",
      "  158  191  193  199  208  235  299  304  308  311  328  329  348  357\n",
      "  360  399  407  418  419  421  422  434  460  463  519  526  545  550\n",
      "  571  576  591  601  603  620  626  640  642  643  660  661  686  711\n",
      "  728  734  735  771  773  804  806  830  857  889  924  937  995 1001\n",
      " 1086 1089 1102 1119 1137 1138 1150 1172 1175 1176 1231 1240 1258 1268\n",
      " 1270 1298 1308 1369 1375 1380 1391 1411 1437 1445 1449 1451 1456 1484\n",
      " 1521 1522 1524 1539 1559 1571 1576 1615 1637 1655 1661 1678 1679 1710\n",
      " 1716 1722 1741 1743 1756 1760 1763 1771 1790 1812 1813 1822 1882 1884\n",
      " 1890 1891 1896 1934 1943 1980 2009 2010 2042 2049 2059 2072 2097 2110\n",
      " 2121 2131 2151 2154 2172 2188 2211 2228 2239 2258 2259 2271 2272 2275\n",
      " 2292 2301 2308 2353 2359 2363 2383 2384 2418 2426 2436 2443 2452 2464\n",
      " 2518 2533 2535 2539 2541 2549 2574 2586 2597 2604 2616 2628 2639 2651\n",
      " 2657 2658 2731 2753 2820 2827 2847 2849 2863 2880 2906 2908 2959 2968\n",
      " 2971 2988 2989 2992 2998 3011 3014 3015 3020 3037 3041 3052 3054 3084\n",
      " 3087 3092 3153 3216 3228 3237 3238 3246 3253 3254 3263 3266 3282 3301\n",
      " 3310 3319 3331 3336 3341 3373 3462 3463 3490 3492 3518 3519 3521 3559\n",
      " 3566 3568 3573 3608 3617 3632 3641 3643 3676 3694 3721 3727]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  30   37   56   58   66   68   78   93   95   97  154  163  186  244\n",
      "  283  292  302  303  312  316  320  330  343  344  371  384  387  394\n",
      "  396  427  466  467  471  479  498  536  539  549  558  564  568  569\n",
      "  609  655  663  687  704  718  721  768  778  798  817  829  839  876\n",
      "  888  910  951  964  967 1036 1066 1108 1120 1124 1127 1128 1130 1152\n",
      " 1159 1167 1188 1191 1193 1213 1221 1230 1235 1307 1395 1397 1405 1413\n",
      " 1418 1420 1421 1442 1470 1489 1494 1496 1502 1503 1510 1512 1534 1537\n",
      " 1594 1597 1607 1620 1621 1625 1627 1630 1633 1634 1639 1641 1652 1698\n",
      " 1702 1707 1745 1765 1789 1797 1805 1810 1820 1858 1900 1916 1926 1930\n",
      " 1935 1942 1955 1976 2016 2020 2026 2033 2064 2082 2085 2086 2092 2139\n",
      " 2149 2153 2155 2169 2179 2186 2191 2192 2227 2248 2265 2314 2317 2333\n",
      " 2339 2342 2347 2355 2366 2373 2377 2379 2411 2417 2419 2420 2429 2430\n",
      " 2438 2451 2486 2490 2499 2517 2578 2582 2591 2619 2621 2632 2644 2650\n",
      " 2655 2668 2670 2672 2689 2707 2728 2747 2784 2825 2874 2894 2924 2931\n",
      " 2938 2942 2947 2996 3017 3024 3036 3043 3089 3134 3140 3142 3150 3156\n",
      " 3159 3160 3169 3172 3178 3210 3243 3249 3277 3305 3313 3332 3343 3362\n",
      " 3363 3364 3367 3372 3408 3410 3447 3466 3481 3483 3505 3512 3514 3534\n",
      " 3576 3593 3605 3640 3669 3673 3685 3708 3716 3724 3725 3734]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  13   20   21   57   59   61   76   90  127  145  157  188  198  202\n",
      "  211  221  222  243  293  314  339  381  386  395  403  413  426  429\n",
      "  453  455  474  477  487  490  500  502  505  523  544  546  554  581\n",
      "  582  618  625  637  641  644  654  681  697  698  699  700  708  714\n",
      "  717  723  740  751  758  759  816  833  851  881  911  921  922  929\n",
      "  934  984  989 1003 1009 1020 1028 1034 1068 1076 1087 1111 1114 1126\n",
      " 1144 1163 1194 1218 1262 1265 1287 1303 1314 1327 1333 1354 1372 1404\n",
      " 1419 1438 1448 1450 1452 1458 1477 1478 1482 1486 1493 1499 1547 1552\n",
      " 1596 1598 1626 1629 1645 1648 1650 1689 1699 1705 1719 1737 1742 1750\n",
      " 1788 1792 1816 1831 1841 1864 1911 1917 1947 1963 1966 1972 2000 2003\n",
      " 2008 2013 2014 2018 2038 2040 2047 2062 2068 2107 2122 2134 2162 2163\n",
      " 2165 2171 2178 2199 2208 2221 2233 2266 2280 2305 2312 2332 2336 2341\n",
      " 2352 2356 2358 2365 2385 2403 2435 2475 2498 2510 2529 2536 2555 2571\n",
      " 2576 2636 2661 2677 2680 2690 2715 2773 2776 2785 2802 2814 2845 2846\n",
      " 2852 2870 2871 2918 2929 2939 2943 2948 2960 3000 3034 3049 3083 3106\n",
      " 3118 3148 3161 3205 3230 3262 3288 3307 3309 3312 3330 3337 3345 3346\n",
      " 3351 3353 3380 3382 3391 3406 3434 3436 3485 3502 3509 3523 3525 3541\n",
      " 3542 3569 3570 3579 3589 3591 3598 3602 3622 3671 3714 3741]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  18   54   79   81   83   85  101  123  125  138  169  171  192  206\n",
      "  215  216  225  228  255  282  284  285  341  388  390  404  408  409\n",
      "  432  470  472  485  493  515  516  537  551  590  599  602  611  633\n",
      "  652  671  684  709  737  738  755  767  769  779  796  814  853  875\n",
      "  886  900  906  917  920  954  955  977  979  992 1007 1012 1023 1039\n",
      " 1062 1088 1091 1100 1104 1155 1164 1216 1220 1223 1238 1252 1254 1285\n",
      " 1302 1309 1347 1358 1371 1393 1409 1412 1435 1436 1441 1444 1460 1474\n",
      " 1495 1501 1509 1528 1550 1560 1567 1588 1595 1614 1619 1649 1653 1665\n",
      " 1673 1692 1693 1703 1709 1728 1740 1749 1751 1794 1802 1808 1857 1859\n",
      " 1865 1889 1895 1903 1931 1933 1938 1952 1959 1979 1992 2019 2043 2044\n",
      " 2076 2099 2104 2138 2152 2206 2217 2218 2220 2237 2270 2274 2306 2322\n",
      " 2357 2361 2372 2405 2437 2467 2502 2520 2527 2543 2559 2567 2581 2606\n",
      " 2613 2666 2691 2724 2730 2749 2756 2760 2783 2807 2808 2815 2816 2824\n",
      " 2834 2838 2844 2850 2856 2869 2876 2885 2913 2927 2928 2933 2963 2972\n",
      " 2983 2987 2995 2999 3019 3022 3055 3056 3059 3071 3088 3096 3100 3101\n",
      " 3125 3139 3145 3157 3167 3194 3233 3270 3271 3298 3303 3352 3360 3365\n",
      " 3374 3384 3401 3413 3440 3442 3448 3479 3508 3539 3567 3577 3603 3604\n",
      " 3610 3613 3627 3642 3645 3656 3658 3693 3699 3700 3709 3732]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [   8   33   42  106  112  149  161  179  180  213  220  224  250  261\n",
      "  262  290  307  318  336  355  401  423  475  503  504  511  513  531\n",
      "  538  541  548  586  589  596  598  608  613  622  665  694  701  702\n",
      "  705  743  760  764  772  805  818  824  828  831  835  856  861  864\n",
      "  877  892  893  903  904  907  908  963  982 1010 1014 1031 1044 1045\n",
      " 1057 1067 1090 1092 1094 1101 1135 1140 1142 1179 1196 1209 1253 1255\n",
      " 1272 1281 1282 1296 1315 1322 1323 1326 1355 1370 1400 1401 1403 1432\n",
      " 1463 1472 1490 1505 1516 1527 1531 1541 1551 1554 1569 1600 1631 1642\n",
      " 1656 1669 1674 1690 1706 1711 1712 1720 1723 1739 1768 1773 1796 1843\n",
      " 1850 1853 1866 1885 1901 1948 1991 2006 2007 2015 2027 2031 2094 2106\n",
      " 2114 2127 2129 2130 2150 2173 2174 2201 2207 2224 2232 2235 2236 2238\n",
      " 2245 2251 2257 2267 2273 2286 2304 2328 2343 2368 2378 2404 2407 2410\n",
      " 2446 2453 2459 2465 2470 2479 2480 2482 2504 2531 2561 2605 2629 2663\n",
      " 2674 2676 2688 2694 2697 2714 2719 2720 2732 2757 2772 2801 2823 2835\n",
      " 2893 2905 2910 2965 2993 3005 3033 3061 3113 3114 3130 3152 3175 3177\n",
      " 3185 3202 3209 3225 3272 3295 3296 3318 3321 3369 3387 3419 3425 3435\n",
      " 3444 3449 3455 3461 3465 3472 3473 3478 3501 3510 3520 3529 3532 3537\n",
      " 3574 3596 3597 3600 3609 3616 3621 3630 3649 3663 3713 3731]\n",
      "TRAIN: [   0    2    3 ... 3744 3745 3747] TEST: [   1   29   36   38   41   43   50   63   73   74   80   91  100  132\n",
      "  136  137  141  150  160  164  205  223  248  271  274  297  301  315\n",
      "  334  335  353  356  359  373  375  385  391  437  443  461  468  473\n",
      "  496  555  559  577  600  606  612  645  646  650  695  724  733  750\n",
      "  797  807  813  823  840  854  859  871  901  914  936  956 1006 1011\n",
      " 1015 1038 1082 1095 1099 1109 1116 1117 1123 1129 1178 1181 1206 1214\n",
      " 1219 1264 1266 1279 1291 1325 1336 1340 1341 1353 1356 1376 1378 1386\n",
      " 1387 1402 1406 1426 1429 1431 1440 1461 1487 1498 1575 1599 1604 1676\n",
      " 1685 1713 1747 1757 1799 1824 1829 1851 1854 1860 1870 1880 1897 1908\n",
      " 1914 1939 1951 1953 1957 1968 1969 1989 1994 2032 2048 2056 2078 2079\n",
      " 2084 2103 2111 2119 2144 2161 2180 2184 2230 2250 2260 2276 2279 2281\n",
      " 2294 2299 2303 2307 2320 2321 2345 2360 2375 2382 2396 2406 2408 2441\n",
      " 2481 2492 2501 2508 2514 2523 2554 2573 2589 2600 2654 2700 2704 2739\n",
      " 2746 2765 2766 2767 2769 2774 2787 2803 2830 2836 2883 2887 2888 2909\n",
      " 2911 2914 2921 2925 2952 2967 2969 2991 3009 3065 3067 3068 3072 3080\n",
      " 3099 3102 3107 3109 3111 3115 3155 3259 3281 3292 3316 3326 3347 3376\n",
      " 3393 3396 3411 3417 3441 3457 3470 3477 3488 3515 3527 3544 3546 3554\n",
      " 3580 3584 3611 3618 3619 3647 3664 3667 3674 3704 3711 3746]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [   6   34   35   46   55   60   70   84   88  121  142  147  156  170\n",
      "  172  173  176  184  185  187  207  253  254  260  264  265  270  275\n",
      "  281  288  289  300  313  333  338  366  374  414  415  425  431  435\n",
      "  444  445  447  494  557  584  597  607  627  628  638  656  668  673\n",
      "  674  682  685  689  727  742  744  745  747  749  780  783  790  791\n",
      "  800  808  821  844  883  938  943  958  983 1030 1032 1049 1055 1061\n",
      " 1103 1110 1113 1145 1149 1160 1162 1168 1195 1205 1208 1215 1224 1227\n",
      " 1232 1245 1251 1257 1260 1318 1321 1352 1408 1427 1433 1549 1553 1579\n",
      " 1611 1628 1644 1647 1660 1663 1696 1755 1762 1766 1777 1786 1807 1819\n",
      " 1826 1844 1845 1872 1928 1932 1971 1987 1999 2001 2036 2057 2069 2088\n",
      " 2091 2112 2133 2141 2145 2177 2213 2216 2255 2288 2309 2311 2316 2326\n",
      " 2334 2344 2364 2369 2386 2390 2395 2461 2496 2505 2515 2516 2519 2522\n",
      " 2525 2551 2570 2585 2598 2599 2608 2614 2620 2623 2642 2646 2653 2669\n",
      " 2673 2695 2727 2763 2779 2786 2809 2831 2841 2859 2860 2873 2912 2917\n",
      " 2934 2958 2975 2978 2994 3008 3018 3025 3032 3042 3058 3073 3103 3127\n",
      " 3147 3181 3192 3198 3207 3214 3218 3224 3244 3276 3285 3304 3320 3322\n",
      " 3339 3354 3370 3402 3404 3426 3432 3450 3506 3528 3530 3533 3556 3578\n",
      " 3595 3612 3624 3631 3633 3634 3636 3653 3666 3690 3723 3736]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [   5   23   87   96  107  174  203  217  232  236  249  259  280  322\n",
      "  364  368  377  393  397  405  439  446  465  481  508  517  525  527\n",
      "  533  560  578  610  636  651  667  675  692  703  725  746  753  777\n",
      "  786  809  846  852  862  863  874  885  913  916  945  946  957  976\n",
      "  985  986  988  991  997 1004 1008 1016 1022 1024 1041 1043 1052 1072\n",
      " 1084 1107 1133 1134 1143 1153 1182 1186 1192 1204 1211 1225 1234 1241\n",
      " 1244 1263 1276 1297 1313 1332 1337 1339 1345 1388 1414 1415 1424 1428\n",
      " 1454 1488 1504 1520 1523 1538 1557 1577 1589 1610 1632 1635 1636 1657\n",
      " 1671 1682 1686 1694 1718 1732 1775 1787 1800 1817 1833 1838 1840 1842\n",
      " 1862 1877 1883 1886 1918 1954 1961 2073 2081 2117 2120 2125 2148 2166\n",
      " 2189 2195 2198 2222 2223 2246 2254 2268 2302 2331 2354 2387 2393 2398\n",
      " 2421 2449 2457 2471 2473 2474 2488 2489 2500 2524 2530 2538 2552 2557\n",
      " 2611 2625 2631 2643 2683 2698 2705 2718 2723 2738 2745 2759 2782 2792\n",
      " 2799 2810 2822 2842 2875 2882 2889 2897 2922 2955 2957 2964 3006 3023\n",
      " 3031 3048 3057 3063 3074 3079 3091 3116 3121 3124 3129 3149 3151 3174\n",
      " 3180 3187 3219 3239 3240 3257 3279 3284 3299 3302 3315 3317 3375 3378\n",
      " 3385 3386 3390 3428 3445 3452 3454 3467 3482 3513 3522 3543 3549 3585\n",
      " 3599 3660 3661 3662 3668 3672 3686 3692 3698 3703 3719 3743]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  28   45   51   65   92  134  166  168  177  201  210  226  230  234\n",
      "  286  323  324  327  331  340  350  351  361  367  370  380  392  411\n",
      "  436  438  442  448  451  482  491  499  509  512  514  521  543  565\n",
      "  593  594  615  616  621  630  634  635  649  662  676  688  719  722\n",
      "  726  739  756  762  825  836  841  848  855  860  896  899  912  918\n",
      "  930  940  968  993 1029 1035 1051 1053 1065 1093 1105 1122 1146 1148\n",
      " 1158 1197 1228 1233 1236 1237 1306 1312 1316 1317 1331 1338 1366 1398\n",
      " 1434 1471 1479 1480 1481 1483 1544 1558 1561 1565 1582 1583 1586 1609\n",
      " 1651 1659 1677 1700 1704 1726 1733 1778 1814 1818 1830 1836 1839 1846\n",
      " 1855 1856 1869 1881 1915 1925 1927 1940 1950 1970 1982 1983 1985 1988\n",
      " 1998 2021 2030 2034 2089 2105 2175 2193 2194 2204 2205 2214 2242 2253\n",
      " 2269 2282 2300 2348 2351 2362 2374 2388 2425 2432 2454 2472 2483 2495\n",
      " 2511 2521 2526 2547 2566 2593 2601 2635 2637 2638 2652 2679 2686 2735\n",
      " 2744 2754 2768 2770 2778 2791 2794 2851 2855 2858 2878 2879 2890 2900\n",
      " 2916 2923 2935 2949 2973 3007 3030 3039 3044 3064 3066 3070 3081 3108\n",
      " 3117 3123 3128 3170 3173 3191 3195 3213 3222 3227 3234 3269 3273 3293\n",
      " 3325 3335 3379 3388 3398 3407 3414 3469 3487 3496 3499 3503 3564 3581\n",
      " 3614 3628 3655 3679 3680 3687 3689 3696 3697 3705 3712 3722]\n",
      "TRAIN: [   0    1    3 ... 3744 3746 3747] TEST: [   2   16   31   69   86  102  119  124  126  152  175  190  219  231\n",
      "  233  241  242  252  258  266  268  278  306  317  326  345  363  365\n",
      "  398  417  430  459  462  478  480  483  534  567  574  587  657  670\n",
      "  707  712  715  729  748  761  766  788  794  812  826  842  867  872\n",
      "  873  919  942  996 1005 1021 1025 1037 1058 1059 1063 1078 1081 1106\n",
      " 1118 1132 1156 1166 1169 1177 1199 1201 1210 1226 1246 1280 1283 1284\n",
      " 1300 1304 1311 1329 1335 1342 1346 1364 1377 1379 1381 1425 1455 1473\n",
      " 1485 1497 1506 1517 1519 1542 1563 1570 1578 1587 1592 1602 1605 1662\n",
      " 1666 1668 1725 1729 1748 1759 1774 1798 1804 1899 1902 1907 1909 1924\n",
      " 1941 1944 1949 1978 1993 2022 2050 2055 2060 2070 2093 2098 2100 2190\n",
      " 2202 2212 2219 2247 2261 2262 2285 2287 2289 2290 2293 2313 2315 2323\n",
      " 2338 2392 2394 2415 2422 2428 2439 2468 2485 2493 2494 2503 2540 2544\n",
      " 2550 2579 2583 2584 2607 2609 2622 2647 2659 2675 2685 2708 2711 2729\n",
      " 2740 2761 2780 2788 2793 2821 2829 2865 2868 2872 2892 2896 2919 2950\n",
      " 2953 2980 2982 3002 3003 3026 3045 3053 3075 3076 3077 3119 3133 3162\n",
      " 3166 3176 3182 3206 3217 3223 3231 3251 3278 3283 3334 3344 3358 3359\n",
      " 3395 3403 3405 3412 3438 3446 3474 3476 3498 3500 3507 3511 3536 3540\n",
      " 3552 3575 3615 3644 3648 3652 3691 3717 3728 3730 3739 3745]\n",
      "TRAIN: [   0    1    2 ... 3744 3745 3746] TEST: [  27   40   75   98  108  111  118  148  165  196  212  251  269  272\n",
      "  295  321  337  372  382  412  433  449  488  489  497  518  522  528\n",
      "  532  562  619  632  639  691  757  763  770  775  784  792  795  802\n",
      "  803  811  819  834  838  850  880  890  895  897  905  931  947  948\n",
      "  950  959  962  969  974  978  999 1019 1048 1064 1069 1070 1080 1112\n",
      " 1157 1174 1190 1212 1217 1239 1261 1290 1299 1310 1343 1348 1349 1360\n",
      " 1392 1416 1430 1465 1525 1526 1532 1535 1566 1573 1584 1601 1617 1624\n",
      " 1654 1658 1734 1746 1769 1828 1832 1863 1871 1873 1874 1904 1905 1919\n",
      " 1929 1945 1960 1962 1967 1973 1977 1995 2012 2051 2071 2090 2108 2113\n",
      " 2126 2136 2147 2159 2167 2170 2181 2200 2203 2231 2249 2283 2296 2319\n",
      " 2324 2380 2399 2409 2416 2423 2434 2440 2442 2447 2456 2458 2478 2497\n",
      " 2507 2512 2532 2534 2553 2580 2590 2595 2596 2602 2615 2640 2665 2678\n",
      " 2687 2699 2702 2709 2710 2713 2722 2733 2734 2751 2758 2781 2811 2817\n",
      " 2828 2832 2840 2843 2862 2895 2915 2920 2926 2930 2944 2976 2979 3010\n",
      " 3012 3027 3035 3040 3062 3078 3110 3112 3137 3146 3164 3171 3183 3200\n",
      " 3221 3235 3242 3245 3247 3255 3258 3261 3264 3289 3291 3329 3342 3348\n",
      " 3377 3389 3397 3416 3424 3427 3431 3459 3497 3526 3545 3548 3550 3558\n",
      " 3583 3623 3654 3670 3683 3684 3701 3707 3715 3720 3740 3747]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  12   19   22   47   62  113  116  122  133  162  183  189  204  227\n",
      "  237  245  246  273  294  349  379  410  428  440  441  486  520  542\n",
      "  547  561  572  573  614  648  659  664  693  785  793  801  810  837\n",
      "  865  866  870  879  923  927  933  944  949  980 1013 1027 1040 1046\n",
      " 1054 1071 1085 1096 1131 1151 1154 1161 1171 1198 1202 1222 1242 1249\n",
      " 1256 1259 1267 1273 1278 1286 1288 1292 1319 1328 1334 1351 1390 1394\n",
      " 1407 1417 1439 1443 1464 1466 1476 1492 1507 1518 1529 1543 1545 1548\n",
      " 1556 1562 1572 1590 1603 1616 1623 1638 1643 1646 1675 1681 1683 1691\n",
      " 1697 1701 1714 1715 1721 1731 1744 1754 1758 1776 1779 1780 1783 1803\n",
      " 1806 1815 1861 1887 1922 1936 1956 1974 1975 1984 1986 2005 2025 2028\n",
      " 2029 2037 2039 2041 2058 2075 2080 2101 2102 2118 2137 2146 2157 2160\n",
      " 2196 2225 2229 2252 2297 2327 2329 2335 2376 2381 2414 2433 2562 2569\n",
      " 2588 2594 2624 2627 2633 2645 2671 2692 2725 2741 2748 2752 2764 2790\n",
      " 2795 2800 2804 2813 2839 2854 2877 2902 2907 2954 2985 2986 3004 3016\n",
      " 3021 3046 3082 3085 3093 3097 3104 3120 3126 3131 3135 3138 3141 3143\n",
      " 3158 3165 3168 3186 3189 3193 3196 3199 3220 3226 3236 3256 3265 3267\n",
      " 3287 3350 3356 3415 3418 3423 3456 3475 3486 3493 3531 3538 3562 3590\n",
      " 3594 3625 3635 3646 3650 3657 3675 3681 3688 3695 3729]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [   3    4    7    9   10   15   17   25   44   52   77  104  114  129\n",
      "  135  139  178  197  200  240  247  256  257  263  305  319  342  347\n",
      "  352  358  362  389  400  402  456  457  458  464  476  495  506  530\n",
      "  552  563  566  570  579  585  592  605  624  647  669  690  731  732\n",
      "  754  815  820  827  843  847  858  869  882  898  902  909  928  935\n",
      "  939  941  952  961  970  971  973  981  987  990 1002 1047 1073 1125\n",
      " 1136 1139 1147 1187 1271 1275 1277 1294 1301 1305 1320 1324 1344 1350\n",
      " 1374 1385 1410 1422 1446 1462 1469 1513 1514 1530 1581 1585 1591 1608\n",
      " 1612 1613 1664 1667 1724 1735 1752 1761 1770 1784 1827 1835 1868 1878\n",
      " 1898 1921 1937 1981 1997 2002 2004 2017 2023 2061 2063 2066 2095 2096\n",
      " 2115 2116 2123 2128 2143 2156 2176 2197 2243 2318 2330 2337 2340 2350\n",
      " 2367 2370 2371 2401 2402 2424 2427 2431 2455 2460 2476 2477 2509 2545\n",
      " 2546 2564 2577 2617 2649 2656 2682 2693 2696 2706 2712 2721 2726 2737\n",
      " 2789 2796 2798 2812 2819 2833 2853 2861 2864 2867 2881 2899 2904 2932\n",
      " 2940 2946 2961 2970 2981 2984 2990 3050 3051 3060 3105 3132 3136 3154\n",
      " 3184 3190 3197 3203 3248 3275 3280 3286 3290 3294 3323 3324 3328 3338\n",
      " 3355 3371 3392 3394 3439 3453 3458 3484 3504 3551 3553 3555 3563 3572\n",
      " 3582 3586 3601 3606 3607 3626 3629 3677 3706 3718 3733]\n",
      "TRAIN: [   0    1    2 ... 3744 3745 3747] TEST: [  13   24   51   57   87   92  102  104  117  127  161  198  201  204\n",
      "  210  217  252  260  266  284  289  292  302  305  314  378  393  404\n",
      "  406  413  419  431  450  480  495  496  508  515  529  540  543  546\n",
      "  547  549  595  651  661  671  684  685  708  710  735  736  741  753\n",
      "  774  795  812  816  824  830  848  856  871  894  988 1000 1050 1063\n",
      " 1064 1072 1079 1107 1121 1127 1131 1154 1223 1278 1318 1322 1337 1354\n",
      " 1366 1400 1402 1447 1468 1471 1481 1492 1539 1550 1560 1599 1616 1618\n",
      " 1623 1629 1648 1655 1663 1665 1672 1678 1689 1743 1777 1788 1852 1871\n",
      " 1878 1893 1905 1908 1914 1920 1928 1932 1963 1996 2037 2038 2042 2049\n",
      " 2053 2058 2059 2068 2078 2080 2085 2088 2091 2124 2129 2139 2152 2156\n",
      " 2180 2193 2207 2218 2247 2253 2261 2283 2287 2291 2296 2313 2317 2359\n",
      " 2377 2378 2392 2419 2421 2430 2441 2464 2478 2481 2482 2527 2534 2582\n",
      " 2609 2612 2625 2629 2660 2679 2705 2734 2741 2751 2782 2803 2808 2810\n",
      " 2828 2861 2881 2925 2956 2966 2972 2977 2985 2987 2991 3020 3051 3068\n",
      " 3072 3078 3081 3083 3094 3124 3125 3133 3159 3176 3187 3194 3208 3246\n",
      " 3251 3260 3289 3297 3300 3323 3334 3338 3340 3348 3350 3351 3387 3393\n",
      " 3419 3425 3438 3443 3472 3473 3475 3476 3489 3492 3504 3510 3515 3525\n",
      " 3532 3553 3570 3607 3622 3632 3636 3640 3682 3714 3729 3746]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  12   17   26   46   56   58   80   91  111  128  143  158  160  167\n",
      "  179  183  224  233  263  275  283  295  307  311  313  339  350  382\n",
      "  401  424  438  440  474  475  482  489  497  509  517  525  557  566\n",
      "  582  586  588  600  606  608  627  634  643  681  697  719  722  749\n",
      "  775  798  823  845  881  884  890  903  909  935  936  937  953  959\n",
      "  971  974  979  980 1011 1035 1059 1074 1080 1093 1160 1161 1168 1179\n",
      " 1180 1196 1201 1214 1239 1267 1283 1346 1348 1370 1372 1383 1388 1416\n",
      " 1443 1470 1496 1514 1543 1545 1548 1557 1587 1598 1609 1614 1643 1646\n",
      " 1659 1682 1725 1727 1745 1802 1805 1808 1836 1868 1870 1887 1892 1898\n",
      " 1903 1906 1938 1973 1985 1986 2015 2024 2033 2086 2093 2119 2121 2140\n",
      " 2145 2148 2162 2164 2185 2187 2189 2198 2224 2254 2263 2274 2277 2279\n",
      " 2281 2338 2344 2351 2354 2368 2384 2389 2401 2411 2425 2429 2440 2457\n",
      " 2459 2461 2491 2492 2522 2541 2546 2562 2569 2576 2584 2587 2595 2601\n",
      " 2615 2617 2636 2638 2662 2673 2682 2689 2690 2698 2720 2726 2732 2756\n",
      " 2772 2798 2832 2858 2880 2883 2941 2960 2963 2971 3004 3008 3012 3028\n",
      " 3030 3035 3038 3040 3057 3074 3075 3106 3114 3137 3171 3202 3221 3229\n",
      " 3243 3271 3283 3320 3356 3359 3431 3470 3511 3513 3536 3543 3546 3555\n",
      " 3573 3578 3580 3599 3609 3615 3634 3637 3643 3648 3692 3697]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [   8   23   31   41   68   75   93   98  116  159  177  192  216  222\n",
      "  223  237  257  261  274  278  301  318  324  338  355  361  370  375\n",
      "  385  394  418  421  422  425  473  483  488  500  533  539  548  553\n",
      "  565  579  614  619  633  725  726  729  756  764  771  776  794  818\n",
      "  854  863  869  877  908  926  933  950  956  973  995 1003 1049 1069\n",
      " 1084 1087 1089 1100 1103 1132 1170 1173 1206 1208 1233 1246 1249 1295\n",
      " 1310 1330 1350 1371 1380 1422 1423 1437 1448 1476 1506 1510 1513 1526\n",
      " 1531 1572 1584 1602 1635 1636 1637 1641 1662 1669 1687 1705 1717 1720\n",
      " 1723 1730 1746 1782 1786 1787 1826 1834 1841 1850 1865 1873 1883 1895\n",
      " 1940 1979 1984 1999 2023 2030 2054 2064 2072 2077 2095 2099 2102 2106\n",
      " 2108 2117 2120 2135 2147 2168 2188 2203 2209 2223 2226 2227 2234 2257\n",
      " 2284 2285 2299 2309 2314 2316 2322 2334 2362 2372 2376 2394 2406 2409\n",
      " 2417 2458 2472 2479 2495 2552 2558 2567 2574 2597 2602 2614 2655 2691\n",
      " 2702 2721 2767 2778 2789 2811 2815 2876 2877 2895 2910 2929 2937 2942\n",
      " 2944 2989 2997 3013 3048 3054 3076 3085 3096 3120 3128 3147 3149 3152\n",
      " 3158 3165 3191 3220 3222 3233 3250 3256 3257 3285 3295 3308 3310 3317\n",
      " 3318 3373 3406 3439 3442 3446 3449 3474 3495 3514 3517 3542 3549 3582\n",
      " 3600 3605 3633 3647 3651 3669 3698 3708 3712 3727 3734 3736]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  15   36   43   60   67   88  101  110  130  139  171  200  226  231\n",
      "  293  321  336  345  396  402  407  415  446  452  458  461  486  492\n",
      "  504  511  522  563  574  604  623  642  665  666  683  688  701  716\n",
      "  720  746  750  791  820  834  842  843  847  850  867  924  946  962\n",
      "  968  972 1012 1033 1042 1044 1047 1086 1105 1106 1110 1120 1123 1124\n",
      " 1129 1162 1167 1169 1174 1193 1194 1200 1203 1215 1219 1240 1248 1272\n",
      " 1274 1291 1296 1321 1329 1341 1361 1363 1368 1379 1386 1393 1399 1420\n",
      " 1421 1449 1451 1455 1512 1518 1532 1556 1564 1574 1583 1585 1589 1656\n",
      " 1675 1685 1721 1759 1771 1811 1814 1817 1829 1837 1860 1917 1936 1941\n",
      " 1947 1966 1970 1975 2021 2025 2027 2052 2098 2112 2123 2128 2165 2208\n",
      " 2217 2249 2255 2267 2282 2295 2325 2331 2337 2340 2369 2385 2398 2426\n",
      " 2484 2502 2514 2518 2538 2544 2564 2571 2593 2608 2611 2621 2650 2651\n",
      " 2661 2683 2707 2713 2728 2747 2757 2781 2793 2795 2796 2805 2821 2833\n",
      " 2836 2838 2853 2856 2906 2908 2909 2911 2931 2932 2955 2961 2992 2994\n",
      " 3021 3023 3042 3052 3065 3073 3077 3082 3093 3098 3117 3122 3150 3151\n",
      " 3157 3160 3205 3213 3240 3247 3274 3277 3282 3293 3306 3333 3365 3372\n",
      " 3392 3396 3398 3407 3409 3420 3424 3448 3480 3496 3500 3518 3527 3558\n",
      " 3566 3569 3627 3631 3678 3681 3689 3709 3732 3737 3738 3744]\n",
      "TRAIN: [   0    1    2 ... 3744 3745 3746] TEST: [  14   37   45   73   74   78   89  106  122  125  142  151  156  175\n",
      "  209  220  238  248  253  270  276  282  296  315  323  326  341  371\n",
      "  392  395  420  429  435  444  445  448  449  465  479  502  507  535\n",
      "  585  610  611  612  624  625  626  635  646  658  660  670  675  690\n",
      "  728  732  763  772  782  787  796  802  807  833  885  886  891  898\n",
      "  916  920  923  932  961  964  969  976  981 1031 1032 1081 1092 1094\n",
      " 1095 1102 1126 1140 1151 1153 1199 1221 1290 1309 1357 1395 1404 1405\n",
      " 1445 1454 1460 1463 1474 1478 1524 1535 1536 1538 1562 1570 1604 1632\n",
      " 1638 1645 1649 1695 1699 1716 1719 1734 1738 1767 1784 1785 1792 1824\n",
      " 1825 1833 1838 1854 1900 1929 1952 1955 1960 1980 1988 2020 2026 2031\n",
      " 2032 2039 2056 2066 2076 2087 2113 2118 2176 2181 2192 2212 2213 2215\n",
      " 2219 2230 2246 2275 2290 2308 2318 2323 2329 2336 2341 2383 2395 2400\n",
      " 2413 2452 2462 2471 2477 2507 2513 2529 2536 2548 2580 2590 2591 2592\n",
      " 2631 2671 2675 2676 2693 2750 2754 2764 2774 2777 2813 2820 2822 2823\n",
      " 2825 2865 2879 2896 2897 2902 2907 2917 2919 2938 2996 3010 3022 3046\n",
      " 3086 3090 3102 3113 3115 3116 3143 3154 3180 3201 3212 3253 3305 3313\n",
      " 3330 3337 3364 3381 3414 3428 3433 3464 3465 3483 3497 3499 3519 3537\n",
      " 3575 3618 3620 3639 3645 3650 3660 3687 3699 3702 3730 3747]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  33   39  137  146  149  153  163  165  193  203  206  219  236  303\n",
      "  328  342  352  364  383  384  391  400  403  423  439  490  505  512\n",
      "  521  523  526  550  552  580  605  607  645  691  700  721  747  754\n",
      "  769  770  779  785  797  799  800  836  857  862  892  902  915  939\n",
      "  942  951  970  986  991 1008 1013 1016 1023 1048 1054 1065 1078 1096\n",
      " 1113 1122 1147 1157 1165 1171 1175 1192 1204 1230 1247 1252 1255 1256\n",
      " 1258 1260 1286 1299 1301 1306 1314 1324 1328 1344 1356 1365 1391 1417\n",
      " 1424 1427 1469 1477 1489 1501 1520 1554 1558 1577 1591 1603 1612 1615\n",
      " 1626 1627 1647 1674 1707 1713 1714 1718 1739 1742 1747 1761 1770 1794\n",
      " 1799 1801 1822 1832 1842 1862 1882 1897 1902 1904 1909 1919 1921 1944\n",
      " 1968 1993 1997 2001 2035 2043 2044 2069 2071 2082 2090 2141 2184 2196\n",
      " 2210 2232 2238 2252 2272 2294 2298 2306 2327 2353 2373 2386 2415 2427\n",
      " 2432 2434 2442 2446 2498 2505 2530 2577 2599 2628 2643 2646 2659 2663\n",
      " 2701 2709 2729 2735 2742 2746 2775 2829 2834 2843 2857 2859 2884 2898\n",
      " 2905 2968 2980 2982 3003 3037 3045 3059 3063 3100 3109 3110 3134 3153\n",
      " 3179 3196 3209 3218 3219 3232 3238 3262 3273 3280 3302 3312 3362 3369\n",
      " 3370 3390 3404 3418 3429 3450 3467 3468 3481 3494 3530 3561 3567 3571\n",
      " 3577 3630 3649 3666 3668 3693 3700 3703 3704 3711 3724 3742]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  18   27   59   81   97  107  120  148  168  172  176  178  197  199\n",
      "  207  221  234  250  297  298  327  351  356  362  366  373  374  381\n",
      "  408  412  417  426  447  477  484  485  519  541  544  554  555  558\n",
      "  590  591  596  601  636  640  652  679  680  705  713  714  718  723\n",
      "  748  780  806  819  838  855  859  870  876  887  940  955  996  999\n",
      " 1014 1020 1039 1040 1041 1046 1066 1109 1116 1163 1181 1185 1190 1191\n",
      " 1222 1232 1261 1270 1271 1275 1280 1289 1307 1311 1317 1323 1340 1351\n",
      " 1355 1375 1390 1407 1426 1441 1497 1544 1578 1625 1634 1639 1658 1679\n",
      " 1690 1692 1700 1701 1704 1709 1711 1751 1754 1774 1775 1800 1807 1835\n",
      " 1858 1880 1888 1894 1913 1918 1930 1933 1948 1961 1971 2005 2009 2016\n",
      " 2018 2057 2070 2073 2094 2153 2173 2175 2177 2233 2289 2297 2311 2321\n",
      " 2333 2346 2347 2350 2356 2367 2487 2488 2490 2493 2497 2501 2503 2554\n",
      " 2559 2561 2563 2578 2586 2600 2623 2627 2654 2665 2694 2697 2711 2724\n",
      " 2738 2768 2792 2846 2847 2899 2901 2915 2921 2924 2946 2948 2958 2973\n",
      " 3007 3019 3047 3089 3091 3123 3146 3162 3166 3169 3174 3185 3188 3193\n",
      " 3225 3227 3239 3242 3255 3265 3272 3281 3284 3322 3329 3335 3358 3360\n",
      " 3361 3366 3367 3377 3378 3411 3423 3469 3516 3539 3544 3547 3554 3574\n",
      " 3610 3611 3626 3646 3655 3667 3674 3688 3690 3701 3710 3717]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [   3    4   11   28   53   71   72   77   85   86  100  108  114  124\n",
      "  141  215  230  242  245  249  255  268  299  340  353  365  368  436\n",
      "  456  457  462  472  499  528  537  542  564  573  587  602  609  617\n",
      "  628  629  631  637  638  641  655  678  693  709  744  757  759  829\n",
      "  831  853  860  864  899  912  952  977  978  990 1001 1002 1015 1036\n",
      " 1043 1062 1076 1114 1155 1176 1184 1217 1220 1225 1292 1294 1297 1326\n",
      " 1335 1338 1343 1358 1369 1389 1392 1401 1408 1425 1450 1452 1464 1475\n",
      " 1484 1494 1515 1522 1527 1552 1561 1565 1600 1620 1633 1653 1666 1708\n",
      " 1710 1740 1741 1773 1776 1798 1813 1827 1844 1846 1867 1896 1910 1911\n",
      " 1924 2055 2063 2083 2096 2104 2122 2149 2154 2163 2166 2172 2178 2190\n",
      " 2194 2235 2243 2264 2276 2286 2303 2324 2360 2408 2437 2439 2466 2489\n",
      " 2500 2508 2511 2524 2535 2537 2539 2547 2570 2572 2605 2630 2633 2648\n",
      " 2666 2680 2717 2718 2723 2727 2730 2743 2752 2766 2785 2787 2809 2814\n",
      " 2831 2835 2860 2867 2870 2873 2885 2892 2894 2900 2926 2927 2928 2939\n",
      " 2953 2957 2970 2998 3016 3018 3027 3031 3033 3036 3064 3111 3170 3197\n",
      " 3210 3214 3234 3236 3248 3254 3266 3278 3287 3307 3309 3316 3346 3376\n",
      " 3383 3401 3413 3441 3447 3457 3458 3471 3490 3491 3502 3509 3523 3564\n",
      " 3581 3588 3606 3608 3625 3672 3677 3686 3723 3728 3735 3740]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [   7    9   48   66   84   95  105  109  133  138  169  174  196  208\n",
      "  246  254  262  285  288  300  306  319  329  335  386  388  399  443\n",
      "  451  468  471  491  494  556  559  583  615  654  669  676  692  703\n",
      "  711  715  731  745  752  760  761  781  784  810  811  822  828  851\n",
      "  865  872  900  907  927  929  941  943  945  984  989  994 1004 1007\n",
      " 1024 1051 1061 1070 1101 1108 1117 1125 1130 1136 1143 1150 1178 1205\n",
      " 1234 1237 1253 1254 1282 1284 1327 1333 1339 1367 1374 1377 1382 1412\n",
      " 1439 1442 1444 1521 1533 1537 1540 1546 1553 1579 1586 1606 1642 1650\n",
      " 1657 1661 1671 1673 1676 1684 1702 1703 1722 1728 1748 1763 1764 1765\n",
      " 1791 1796 1809 1812 1815 1821 1839 1840 1843 1866 1875 1877 1884 1891\n",
      " 1935 1942 1946 1951 1953 1992 1994 2011 2013 2062 2107 2109 2111 2114\n",
      " 2130 2142 2157 2200 2204 2228 2248 2266 2326 2335 2361 2370 2374 2447\n",
      " 2454 2456 2474 2486 2506 2520 2523 2585 2607 2622 2652 2684 2686 2692\n",
      " 2716 2740 2748 2791 2799 2800 2806 2818 2842 2848 2862 2871 2872 2874\n",
      " 2875 2923 2962 2974 2978 3000 3032 3043 3049 3058 3103 3127 3131 3141\n",
      " 3155 3203 3211 3217 3224 3235 3244 3263 3267 3298 3311 3321 3324 3371\n",
      " 3379 3389 3395 3403 3430 3452 3455 3477 3505 3522 3529 3534 3548 3560\n",
      " 3563 3593 3594 3602 3616 3652 3657 3664 3694 3713 3715 3721]\n",
      "TRAIN: [   1    2    3 ... 3744 3746 3747] TEST: [   0   19   22   29   35   47   62   76  113  118  129  131  140  144\n",
      "  182  194  228  229  232  240  247  251  272  273  281  310  317  322\n",
      "  325  333  343  347  357  389  405  411  430  433  434  442  455  493\n",
      "  501  503  516  536  578  618  620  647  677  682  695  696  707  742\n",
      "  751  768  793  808  813  827  861  873  882  901  904  911  922  948\n",
      "  954  957 1006 1018 1019 1022 1026 1029 1037 1038 1097 1128 1158 1159\n",
      " 1172 1188 1198 1224 1228 1236 1241 1263 1353 1360 1394 1414 1428 1434\n",
      " 1456 1472 1473 1483 1499 1516 1563 1569 1592 1617 1621 1628 1640 1683\n",
      " 1693 1694 1697 1733 1750 1768 1778 1790 1831 1853 1881 1889 1950 1958\n",
      " 1974 1995 2012 2041 2060 2065 2067 2092 2097 2105 2131 2134 2137 2146\n",
      " 2160 2220 2221 2229 2242 2251 2269 2307 2310 2320 2342 2345 2352 2371\n",
      " 2381 2390 2393 2397 2418 2431 2438 2455 2515 2517 2542 2549 2550 2556\n",
      " 2560 2575 2598 2634 2641 2668 2674 2688 2699 2700 2704 2708 2731 2737\n",
      " 2739 2745 2753 2759 2801 2812 2837 2845 2852 2904 2949 2952 2964 2981\n",
      " 2984 3001 3014 3024 3025 3108 3138 3142 3144 3167 3168 3173 3182 3190\n",
      " 3198 3199 3204 3215 3258 3286 3291 3292 3294 3315 3336 3339 3341 3343\n",
      " 3394 3405 3422 3432 3434 3445 3454 3479 3487 3501 3506 3507 3512 3533\n",
      " 3557 3576 3579 3586 3587 3613 3617 3659 3663 3695 3707 3745]\n",
      "TRAIN: [   0    1    3 ... 3745 3746 3747] TEST: [   2   25   50   55   61   99  135  145  147  152  155  184  189  190\n",
      "  191  195  211  213  239  256  286  330  363  379  414  432  437  454\n",
      "  527  562  568  571  572  577  592  598  639  662  664  717  727  737\n",
      "  758  773  777  783  809  825  841  844  849  858  888  896  897  917\n",
      "  918  930  934  944  947  958  960  967 1028 1052 1053 1055 1073 1077\n",
      " 1098 1115 1139 1149 1152 1156 1166 1177 1235 1238 1243 1244 1268 1305\n",
      " 1312 1316 1342 1352 1359 1362 1373 1381 1384 1418 1433 1461 1462 1485\n",
      " 1502 1503 1547 1551 1568 1573 1575 1593 1607 1610 1668 1680 1681 1724\n",
      " 1726 1735 1744 1753 1755 1769 1823 1828 1845 1855 1857 1863 1899 1922\n",
      " 1925 1939 1954 1956 1957 2000 2002 2007 2017 2028 2045 2050 2110 2116\n",
      " 2136 2144 2182 2186 2191 2201 2214 2222 2236 2239 2240 2250 2262 2278\n",
      " 2280 2300 2328 2355 2365 2366 2399 2407 2420 2428 2444 2451 2463 2480\n",
      " 2496 2519 2525 2545 2589 2603 2613 2618 2620 2626 2639 2649 2653 2733\n",
      " 2744 2771 2776 2780 2788 2794 2802 2816 2827 2830 2851 2869 2882 2914\n",
      " 2922 2933 2945 2959 2986 3015 3034 3044 3050 3053 3055 3056 3060 3062\n",
      " 3079 3099 3126 3139 3163 3181 3186 3192 3200 3207 3216 3223 3230 3264\n",
      " 3276 3290 3301 3319 3385 3417 3421 3426 3482 3484 3485 3488 3535 3538\n",
      " 3551 3591 3604 3635 3656 3658 3662 3665 3671 3706 3731 3741]\n",
      "TRAIN: [   0    2    3 ... 3745 3746 3747] TEST: [   1    5   44   65   70   79   83   96  103  123  134  150  154  180\n",
      "  235  241  259  267  291  320  331  332  358  369  390  397  459  463\n",
      "  498  506  514  518  530  560  599  632  648  649  663  674  698  702\n",
      "  730  733  743  789  792  803  804  805  832  840  868  879  889  893\n",
      "  895  919  928  966  982  987 1005 1067 1075 1082 1083 1090 1141 1142\n",
      " 1207 1210 1226 1242 1245 1250 1273 1285 1287 1303 1320 1331 1349 1364\n",
      " 1378 1385 1410 1411 1419 1429 1430 1438 1440 1457 1466 1490 1493 1530\n",
      " 1549 1559 1576 1582 1596 1597 1613 1686 1688 1698 1731 1752 1795 1810\n",
      " 1847 1849 1859 1864 1879 1885 1937 1943 1949 1959 1965 1967 1976 1977\n",
      " 1981 1982 1991 2003 2004 2022 2040 2061 2074 2143 2151 2155 2161 2179\n",
      " 2195 2245 2259 2270 2271 2293 2302 2305 2315 2349 2363 2382 2388 2403\n",
      " 2424 2445 2448 2450 2467 2470 2473 2483 2512 2516 2528 2531 2540 2543\n",
      " 2557 2568 2594 2606 2610 2616 2624 2635 2647 2687 2703 2722 2763 2779\n",
      " 2784 2786 2819 2826 2840 2855 2863 2868 2886 2890 2893 2943 2975 2979\n",
      " 2999 3006 3026 3039 3067 3070 3092 3104 3107 3136 3140 3145 3156 3175\n",
      " 3183 3184 3189 3241 3249 3269 3303 3304 3314 3326 3327 3328 3331 3332\n",
      " 3344 3357 3368 3388 3412 3437 3440 3460 3498 3521 3524 3528 3540 3585\n",
      " 3589 3590 3596 3619 3641 3653 3654 3661 3670 3691 3718 3743]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [   6   10   16   20   32   42   52   63   82   90   94  166  187  212\n",
      "  225  227  244  269  279  348  349  380  410  416  427  428  441  453\n",
      "  460  469  481  520  524  534  545  551  594  613  644  668  673  704\n",
      "  706  724  739  765  767  788  790  801  814  817  852  874  875  878\n",
      "  914  938  963  965  985  992  993 1025 1057 1060 1091 1099 1133 1135\n",
      " 1137 1138 1144 1182 1197 1202 1209 1212 1229 1251 1257 1264 1276 1281\n",
      " 1288 1298 1315 1319 1345 1397 1403 1406 1409 1446 1453 1467 1487 1488\n",
      " 1500 1519 1525 1528 1529 1541 1542 1566 1567 1580 1588 1594 1601 1608\n",
      " 1611 1619 1630 1644 1651 1652 1715 1737 1757 1762 1766 1772 1797 1803\n",
      " 1856 1876 1901 1915 1916 1923 1927 1945 1964 1978 1983 1987 1990 2006\n",
      " 2010 2014 2019 2029 2034 2036 2051 2079 2089 2126 2170 2174 2183 2199\n",
      " 2205 2211 2216 2225 2237 2241 2268 2343 2387 2412 2436 2443 2469 2504\n",
      " 2510 2551 2553 2555 2573 2583 2588 2604 2645 2658 2669 2670 2706 2710\n",
      " 2712 2714 2736 2749 2760 2762 2765 2769 2824 2850 2866 2888 2891 2912\n",
      " 2916 2936 2950 2951 2967 2976 3029 3088 3118 3119 3148 3164 3177 3231\n",
      " 3252 3275 3299 3325 3345 3353 3354 3374 3380 3384 3391 3397 3399 3400\n",
      " 3402 3416 3427 3444 3456 3459 3461 3462 3531 3550 3556 3572 3584 3595\n",
      " 3598 3612 3629 3642 3673 3679 3680 3685 3696 3719 3720 3725]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  21   30   38  112  126  132  136  162  170  181  186  188  202  205\n",
      "  214  218  243  264  277  280  294  312  316  334  359  360  367  387\n",
      "  398  409  470  478  487  510  513  532  538  570  581  597  603  616\n",
      "  622  650  656  657  659  672  686  694  699  712  762  766  778  821\n",
      "  826  835  837  839  846  905  913  921  949  975  983  997  998 1027\n",
      " 1030 1034 1056 1058 1068 1071 1085 1088 1112 1118 1134 1148 1164 1211\n",
      " 1213 1216 1259 1265 1277 1279 1293 1300 1302 1304 1308 1325 1334 1336\n",
      " 1431 1436 1465 1479 1480 1491 1495 1498 1504 1505 1507 1508 1509 1571\n",
      " 1595 1605 1622 1631 1654 1664 1706 1729 1732 1736 1780 1783 1793 1804\n",
      " 1819 1851 1861 1874 1886 1890 1912 1926 1934 1962 1998 2081 2115 2125\n",
      " 2132 2133 2138 2150 2159 2167 2171 2197 2202 2206 2231 2256 2258 2260\n",
      " 2265 2273 2288 2292 2301 2312 2319 2330 2332 2339 2348 2357 2375 2404\n",
      " 2410 2422 2423 2449 2468 2475 2532 2533 2579 2581 2619 2640 2642 2644\n",
      " 2656 2664 2672 2677 2685 2696 2773 2783 2790 2804 2817 2841 2887 2889\n",
      " 2913 2918 2930 2940 2954 2965 2988 2990 2995 3005 3017 3041 3069 3080\n",
      " 3084 3095 3097 3105 3121 3132 3172 3178 3228 3237 3270 3279 3296 3342\n",
      " 3349 3355 3363 3386 3410 3435 3436 3478 3486 3503 3520 3541 3545 3559\n",
      " 3568 3597 3614 3644 3675 3676 3683 3705 3726 3733 3739]\n",
      "TRAIN: [   0    1    2 ... 3745 3746 3747] TEST: [  34   40   49   54   64   69  115  119  121  157  164  173  185  258\n",
      "  265  271  287  290  304  308  309  337  344  346  354  372  376  377\n",
      "  464  466  467  476  531  561  567  569  575  576  584  589  593  621\n",
      "  630  653  667  687  689  734  738  740  755  786  815  866  880  883\n",
      "  906  910  925  931 1009 1010 1017 1021 1045 1104 1111 1119 1145 1146\n",
      " 1183 1186 1187 1189 1195 1218 1227 1231 1262 1266 1269 1313 1332 1347\n",
      " 1376 1387 1396 1398 1413 1415 1432 1435 1458 1459 1482 1486 1511 1517\n",
      " 1523 1534 1555 1581 1590 1624 1660 1667 1670 1677 1691 1696 1712 1749\n",
      " 1756 1758 1760 1779 1781 1789 1806 1816 1818 1820 1830 1848 1869 1872\n",
      " 1907 1931 1969 1972 1989 2008 2046 2047 2048 2075 2084 2100 2101 2103\n",
      " 2127 2158 2169 2244 2304 2358 2364 2379 2380 2391 2396 2402 2405 2414\n",
      " 2416 2433 2435 2453 2460 2465 2476 2485 2494 2499 2509 2521 2526 2565\n",
      " 2566 2596 2632 2637 2657 2667 2678 2681 2695 2715 2719 2725 2755 2758\n",
      " 2761 2770 2797 2807 2839 2844 2849 2854 2864 2878 2903 2920 2934 2935\n",
      " 2947 2969 2983 2993 3002 3009 3011 3061 3066 3071 3087 3101 3112 3129\n",
      " 3130 3135 3161 3195 3206 3226 3245 3259 3261 3268 3288 3347 3352 3375\n",
      " 3382 3408 3415 3451 3453 3463 3466 3493 3508 3526 3552 3562 3565 3583\n",
      " 3592 3601 3603 3621 3623 3624 3628 3638 3684 3716 3722]\n"
     ]
    }
   ],
   "source": [
    "# F1 SCORE\n",
    "\n",
    "dataframe = pd.read_csv('PQ_G1.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2','PAG_NAME_3','PAG_NAME_4'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=5, random_state=1)\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    model = ExtraTreesClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    yhat = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, yhat)\n",
    "    print('Accuracy for PQ_G1 is',round((100*accuracy),2))\n",
    "    f1 = f1_score(y_test, yhat,average='weighted')\n",
    "    print('F1 score for PQ_G1 is',round((100*f1),2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for PQ_G2 is 86.92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('PQ_G2.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=5, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "print('Accuracy for PQ_G2 is',round((100*mean(n_scores)),2))\n",
    "accs.append(100*mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ETC_PQ_G2.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for PQ_G3 is 82.08\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-2d1db84973d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy for PQ_G3 is'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0maccs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accs' is not defined"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv('PQ_G3.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2','PAG_NAME_3'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=5, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "print('Accuracy for PQ_G3 is',round((100*mean(n_scores)),2))\n",
    "accs.append(100*mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ETC_PQ_G3.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for PQ_G3 is 77.66\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-d23ac1a50683>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy for PQ_G3 is'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0maccs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accs' is not defined"
     ]
    }
   ],
   "source": [
    "# WITHOUT UEF\n",
    "\n",
    "dataframe = pd.read_csv('PQ_G3_without_UEF.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=5, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "print('Accuracy for PQ_G3 is',round((100*mean(n_scores)),2))\n",
    "accs.append(100*mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for PQ_G4 is 85.81\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier()"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('PQ_G4.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=5, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "print('Accuracy for PQ_G4 is',round((100*mean(n_scores)),2))\n",
    "accs.append(100*mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ETC_PQ_G4.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MT_G1 is 62.64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier()"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G1.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=5, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "print('Accuracy for MT_G1 is',round((100*mean(n_scores)),2))\n",
    "accs.append(100*mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ETC_MT_G1.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MT_G2 is 58.77\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier()"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G2.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=5, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "print('Accuracy for MT_G2 is',round((100*mean(n_scores)),2))\n",
    "accs.append(100*mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ETC_MT_G2.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MT_G3 is 76.44\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier()"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G3.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=5, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "print('Accuracy for MT_G3 is',round((100*mean(n_scores)),2))\n",
    "accs.append(100*mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ETC_MT_G3.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MT_G3 is 78.41\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-0bb929712967>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy for MT_G3 is'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0maccs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accs' is not defined"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G3_without_MOCA.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=5, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "print('Accuracy for MT_G3 is',round((100*mean(n_scores)),2))\n",
    "accs.append(100*mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MT_G4 is 62.7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G4.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "datax = X.values\n",
    "datay = y.values\n",
    "\n",
    "X = datax.astype(str)\n",
    "y = datay.astype(str)\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=5, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "print('Accuracy for MT_G4 is',round((100*mean(n_scores)),2))\n",
    "accs.append(100*mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ETC_MT_G4.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MT_G5 is 63.17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G5.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=15, n_repeats=5, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "print('Accuracy for MT_G5 is',round((100*mean(n_scores)),2))\n",
    "accs.append(100*mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ETC_MT_G5.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[91.18161635491515, 87.61079273460226, 82.36781609195401, 85.81178365065834, 62.64152030818697, 58.7742504409171, 76.44065327978582, 62.697674418604656, 63.167179487179496]\n",
      "Average accuracy of Extra Trees Classifier is = 74.52\n"
     ]
    }
   ],
   "source": [
    "print(accs)\n",
    "print('Average accuracy of Extra Trees Classifier is =',round(mean(accs),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## XGBOOST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:48:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "<class 'numpy.ndarray'> (979, 70)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy for PQ_G1 is 92.13\n",
      "[14:48:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('PQ_G1.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2','PAG_NAME_3','PAG_NAME_4'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "model = XGBClassifier(\"binary:logistic\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(type(X_test), X_test.shape)\n",
    "print(X_test)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "    \n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('Accuracy for PQ_G1 is',round((100*accuracy),2))\n",
    "accs.append(accuracy)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('XGBC_PQ_G1.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:27:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy for PQ_G2 is 88.7\n",
      "[14:27:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('PQ_G2.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    " \n",
    "model = XGBClassifier(\"binary:logistic\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "    \n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('Accuracy for PQ_G2 is',round((100*accuracy),2))\n",
    "accs.append(accuracy)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('XGBC_PQ_G2.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:11:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy for PQ_G3 is 80.23\n",
      "[14:11:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('PQ_G3.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2','PAG_NAME_3'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "model = XGBClassifier(\"binary:logistic\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "    \n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('Accuracy for PQ_G3 is',round((100*accuracy),2))\n",
    "accs.append(accuracy)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('XGBC_PQ_G3.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:12:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy for PQ_G3 is 75.58\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7c0ac46c9665>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy for PQ_G3 is'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0maccs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accs' is not defined"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv('PQ_G3_without_UEF.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "model = XGBClassifier(\"binary:logistic\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "    \n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('Accuracy for PQ_G3 is',round((100*accuracy),2))\n",
    "accs.append(accuracy)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:11:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy for PQ_G4 is 86.29\n",
      "[14:11:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('PQ_G4.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "model = XGBClassifier(\"binary:logistic\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "    \n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('Accuracy for PQ_G4 is',round((100*accuracy),2))\n",
    "accs.append(accuracy)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('XGBC_PQ_G4.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:12:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy for MT_G1 is 61.19\n",
      "[14:12:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G1.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "model = XGBClassifier(\"binary:logistic\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "    \n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('Accuracy for MT_G1 is',round((100*accuracy),2))\n",
    "accs.append(accuracy)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('XGBC_MT_G1.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:12:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy for MT_G2 is 59.04\n",
      "[14:12:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G2.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    \n",
    "\n",
    "model = XGBClassifier(\"binary:logistic\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "    \n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('Accuracy for MT_G2 is',round((100*accuracy),2))\n",
    "accs.append(accuracy)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('XGBC_MT_G2.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:12:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy for MT_G3 is 80.27\n",
      "[14:12:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G3.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    \n",
    "\n",
    "model = XGBClassifier(\"binary:logistic\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "    \n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('Accuracy for MT_G3 is',round((100*accuracy),2))\n",
    "accs.append(accuracy)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('XGBC_MT_G3.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:14:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy for MT_G3 is 79.73\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-f600196435a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy for MT_G3 is'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0maccs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accs' is not defined"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G3_without_MOCA.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    \n",
    "\n",
    "model = XGBClassifier(\"binary:logistic\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "    \n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('Accuracy for MT_G3 is',round((100*accuracy),2))\n",
    "accs.append(accuracy)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:12:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy for MT_G4 is 53.49\n",
      "[14:12:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G4.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "datax = X.values\n",
    "datay = y.values\n",
    "\n",
    "X = datax.astype(str)\n",
    "y = datay.astype(str)\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "model = XGBClassifier(\"binary:logistic\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "    \n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('Accuracy for MT_G4 is',round((100*accuracy),2))\n",
    "accs.append(accuracy)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('XGBC_MT_G4.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:12:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy for MT_G5 is 62.34\n",
      "[14:12:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G5.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "datax = X.values\n",
    "datay = y.values\n",
    "\n",
    "X = datax.astype(str)\n",
    "y = datay.astype(str)\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "model = XGBClassifier(\"binary:logistic\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "    \n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('Accuracy for MT_G5 is',round((100*accuracy),2))\n",
    "accs.append(accuracy)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('XGBC_MT_G5.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9213483146067416, 0.8876543209876543, 0.8023255813953488, 0.8629441624365483, 0.9213483146067416, 0.9213483146067416, 0.8023255813953488, 0.9213483146067416, 0.8876543209876543, 0.8023255813953488, 0.8023255813953488, 0.8629441624365483, 0.6119235095613048, 0.5903614457831325, 0.8026666666666666, 0.5348837209302325, 0.6233766233766234] \n",
      "\n",
      "Average accuracy of XGBClassifier is = 79.76\n"
     ]
    }
   ],
   "source": [
    "print(accs,\"\\n\")\n",
    "print('Average accuracy of XGBClassifier is =',round(100*mean(accs),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for PQ_G1 is 90.34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('PQ_G1.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2','PAG_NAME_3','PAG_NAME_4'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\n",
    "print('Accuracy for PQ_G1 is',round((100*mean(n_scores)),2))\n",
    "accs.append(100*mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RFC_PQ_G1.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for PQ_G2 is 86.99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('PQ_G2.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\n",
    "print('Accuracy for PQ_G2 is',round((100*mean(n_scores)),2))\n",
    "accs.append(100*mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RFC_PQ_G2.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for PQ_G3 is 82.48\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('PQ_G3.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2','PAG_NAME_3'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\n",
    "print('Accuracy for PQ_G3 is',round((100*mean(n_scores)),2))\n",
    "accs.append(100*mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RFC_PQ_G3.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for PQ_G4 is 85.81\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('PQ_G4.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\n",
    "print('Accuracy for PQ_G4 is',round((100*mean(n_scores)),2))\n",
    "accs.append(100*mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RFC_PQ_G4.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MT_G1 is 63.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G1.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "    \n",
    "datax = X.values\n",
    "datay = y.values\n",
    "X = datax.astype(str)\n",
    "y = datay.astype(str)\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy for MT_G1 is',round((100*mean(n_scores)),2))\n",
    "accs.append(100*mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RFC_MT_G1.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MT_G2 is 62.61\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G2.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "    \n",
    "datax = X.values\n",
    "datay = y.values\n",
    "X = datax.astype(str)\n",
    "y = datay.astype(str)\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy for MT_G2 is',round((100*mean(n_scores)),2))\n",
    "accs.append(100*mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RFC_MT_G2.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MT_G3 is 77.48\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G3.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "    \n",
    "datax = X.values\n",
    "datay = y.values\n",
    "X = datax.astype(str)\n",
    "y = datay.astype(str)\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy for MT_G3 is',round((100*mean(n_scores)),2))\n",
    "accs.append(100*mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RFC_MT_G3.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MT_G3 is 78.4\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-4a5511e82da1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mn_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'raise'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy for MT_G3 is'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0maccs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accs' is not defined"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G3_without_MOCA.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "    \n",
    "datax = X.values\n",
    "datay = y.values\n",
    "X = datax.astype(str)\n",
    "y = datay.astype(str)\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy for MT_G3 is',round((100*mean(n_scores)),2))\n",
    "accs.append(100*mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MT_G4 is 64.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G4.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "    \n",
    "datax = X.values\n",
    "datay = y.values\n",
    "X = datax.astype(str)\n",
    "y = datay.astype(str)\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy for MT_G4 is',round((100*mean(n_scores)),2))\n",
    "accs.append(100*mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RFC_MT_G4.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MT_G5 is 64.78\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G5.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\n",
    "print('Accuracy for MT_G5 is',round((100*mean(n_scores)),2))\n",
    "accs.append(100*mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RFC_MT_G5.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90.34219495569185, 87.59508004089793, 82.48246585455888, 85.8117624787026, 63.246448695886905, 62.61324041811849, 77.48080808080807, 64.59535256410257, 64.77732793522267] \n",
      "\n",
      "Average accuracy of Random Forest Classifier is = 75.44\n"
     ]
    }
   ],
   "source": [
    "print(accs,\"\\n\")\n",
    "print('Average accuracy of Random Forest Classifier is =',round(mean(accs),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for PQ_G1 is 90.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('PQ_G1.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2','PAG_NAME_3','PAG_NAME_4'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "# datax = X.values\n",
    "# datay = y.values\n",
    "\n",
    "# X = datax.astype(str)\n",
    "# y = datay.astype(str)\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy for PQ_G1 is',round((100*accuracy),2))\n",
    "accs.append(accuracy)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LR_PQ_G1.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for PQ_G2 is 87.04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('PQ_G2.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy for PQ_G2 is',round((100*accuracy),2))\n",
    "accs.append(accuracy)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LR_PQ_G2.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for PQ_G3 is 77.91\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('PQ_G3.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2','PAG_NAME_3'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy for PQ_G3 is',round((100*accuracy),2))\n",
    "accs.append(accuracy)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LR_PQ_G3.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for PQ_G4 is 86.29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('PQ_G4.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy for PQ_G4 is',round((100*accuracy),2))\n",
    "accs.append(accuracy)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LR_PQ_G4.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MT_G1 is 61.98\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G1.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy for MT_G1 is',round((100*accuracy),2))\n",
    "accs.append(accuracy)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LR_MT_G1.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MT_G2 is 62.65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G2.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy for MT_G2 is',round((100*accuracy),2))\n",
    "accs.append(accuracy)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LR_MT_G2.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MT_G3 is 80.8\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-8d696aa3648d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy for MT_G3 is'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0maccs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accs' is not defined"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G3.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy for MT_G3 is',round((100*accuracy),2))\n",
    "accs.append(accuracy)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LR_MT_G3.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MT_G3 is 80.13\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-bee71bea350a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy for MT_G3 is'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0maccs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accs' is not defined"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G3_without_MOCA.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy for MT_G3 is',round((100*accuracy),2))\n",
    "accs.append(accuracy)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MT_G4 is 62.79\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G4.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "datax = X.values\n",
    "datay = y.values\n",
    "X = datax.astype(str)\n",
    "y = datay.astype(str)\n",
    "    \n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy for MT_G4 is',round((100*accuracy),2))\n",
    "accs.append(accuracy)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LR_MT_G4.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MT_G5 is 70.13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G5.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy for MT_G5 is',round((100*accuracy),2))\n",
    "accs.append(accuracy)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LR_MT_G5.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9039836567926456, 0.8765432098765432, 0.7790697674418605, 0.8629441624365483, 0.6197975253093363, 0.6265060240963856, 0.808, 0.627906976744186, 0.7012987012987013] \n",
      "\n",
      "Average accuracy of Logistic Regression is = 75.62\n"
     ]
    }
   ],
   "source": [
    "print(accs,\"\\n\")\n",
    "print('Average accuracy of Logistic Regression is =',round(100*mean(accs),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## LIGHTGBM CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for PQ_G1 is 91.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('PQ_G1.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2','PAG_NAME_3','PAG_NAME_4'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "    \n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = LGBMClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "print('Accuracy for PQ_G1 is',round((100*mean(n_scores)),2))\n",
    "accs.append(mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LGBMC_PQ_G1.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for PQ_G2 is 87.89\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('PQ_G2.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = LGBMClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "print('Accuracy for PQ_G2 is',round((100*mean(n_scores)),2))\n",
    "accs.append(mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LGBMC_PQ_G2.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for PQ_G3 is 83.49\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('PQ_G3.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2','PAG_NAME_3'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = LGBMClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "print('Accuracy for PQ_G3 is',round((100*mean(n_scores)),2))\n",
    "accs.append(mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LGBMC_PQ_G3.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for PQ_G4 is 85.81\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('PQ_G4.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = LGBMClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "print('Accuracy for PQ_G4 is',round((100*mean(n_scores)),2))\n",
    "accs.append(mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LGBMC_PQ_G4.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MT_G1 is 64.06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G1.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = LGBMClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "print('Accuracy for MT_G1 is',round((100*mean(n_scores)),2))\n",
    "accs.append(mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LGBMC_MT_G1.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MT_G2 is 57.7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G2.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = LGBMClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "print('Accuracy for MT_G2 is',round((100*mean(n_scores)),2))\n",
    "accs.append(mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LGBMC_MT_G2.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MT_G3 is 79.28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G3.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = LGBMClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "print('Accuracy for MT_G3 is',round((100*mean(n_scores)),2))\n",
    "accs.append(mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MT_G3 is 78.67\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-28266306cf8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy for MT_G3 is'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0maccs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accs' is not defined"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G3_without_MOCA.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = LGBMClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "print('Accuracy for MT_G3 is',round((100*mean(n_scores)),2))\n",
    "accs.append(mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LGBMC_MT_G3.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MT_G4 is 62.64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G4.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME_1','PAG_NAME_2'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "datax = X.values\n",
    "datay = y.values\n",
    "\n",
    "X = datax.astype(str)\n",
    "y = datay.astype(str)\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = LGBMClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "print('Accuracy for MT_G4 is',round((100*mean(n_scores)),2))\n",
    "accs.append(mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LGBMC_MT_G4.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MT_G5 is 62.38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('MT_G5.csv')\n",
    "X = dataframe.drop(columns=['RECRUITMENT_CAT','PATNO','EVENT_ID','PAG_NAME'],axis=1) \n",
    "y = dataframe['RECRUITMENT_CAT']\n",
    "\n",
    "X = ordinal.fit_transform(X)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "model = LGBMClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "print('Accuracy for MT_G5 is',round((100*mean(n_scores)),2))\n",
    "accs.append(mean(n_scores))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LGBMC_MT_G5.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9130210202690482, 0.8838532558104046, 0.834939091915836, 0.858117624787026, 0.6405623038769105, 0.5770034843205575, 0.792774569221628, 0.6263621794871795, 0.623774179037337] \n",
      "\n",
      "Average accuracy of LGBMClassifier is = 75.0\n"
     ]
    }
   ],
   "source": [
    "print(accs,\"\\n\")\n",
    "print('Average accuracy of LGBMClassifier is =',round(100*mean(accs),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
